{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5CXS1Mpthia"
      },
      "source": [
        "Reference\n",
        "- [Github](https://github.com/berniwal/swin-transformer-pytorch/blob/master/swin_transformer_pytorch/swin_transformer.py)\n",
        "- [Youtube](https://www.youtube.com/playlist?list=PL9iXGo3xD8jokWaLB8ZHUkjjv5Y_vPQnZ)\n",
        "# To Do\n",
        "- ✅ Delete Shifted Swin Transformer from Swin Block\n",
        "- ✅ Dual Switch\n",
        "- ✅ CNN\n",
        "- ✅ Edit Function to support CNN\n",
        "- Hybrid Network Backbone\n",
        "- ✅ Hyneter Module\n",
        "- Extract Feature from model [Pytorch | Feature extraction for model inspection](https://docs.pytorch.org/vision/main/feature_extraction.html)\n",
        "- Modify Masked R-CNN backbone [TorchVision Object Detection Finetuning Tutorial](https://docs.pytorch.org/tutorials/intermediate/torchvision_tutorial.html#modifying-the-model-to-add-a-different-backbone)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcDRsMs1tnGg"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zpj3gv59shPE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylDZJ8Uns6L-"
      },
      "source": [
        "## Residul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c8DLTBNutBx5"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCs-x-BTvUT3"
      },
      "source": [
        "## Pre Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xGTZbg1HxaIh"
      },
      "outputs": [],
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jJEutgIxe8I"
      },
      "source": [
        "## Feed Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G8WP0AyGxbzl"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def create_mask(window_size, displacement, upper_lower, left_right):\n",
        "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
        "\n",
        "    if upper_lower:\n",
        "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
        "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
        "\n",
        "    if left_right:\n",
        "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
        "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
        "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
        "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def get_relative_distances(window_size):\n",
        "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
        "    distances = indices[None, :, :] - indices[:, None, :]\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Window Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        inner_dim = head_dim * heads\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.window_size = window_size\n",
        "        self.relative_pos_embedding = relative_pos_embedding\n",
        "        self.shifted = shifted\n",
        "\n",
        "        if self.shifted:\n",
        "            displacement = window_size // 2\n",
        "            self.cyclic_shift = CyclicShift(-displacement)\n",
        "            self.cyclic_back_shift = CyclicShift(displacement)\n",
        "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
        "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
        "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
        "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "        if self.relative_pos_embedding:\n",
        "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
        "        else:\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
        "\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "       \n",
        "    def forward(self, x):\n",
        "        if self.shifted:\n",
        "            x = self.cyclic_shift(x)\n",
        "\n",
        "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        nw_h = n_h // self.window_size\n",
        "        nw_w = n_w // self.window_size\n",
        "\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
        "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
        "\n",
        "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
        "\n",
        "        if self.relative_pos_embedding:\n",
        "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
        "        else:\n",
        "            dots += self.pos_embedding\n",
        "\n",
        "        if self.shifted:\n",
        "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
        "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
        "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
        "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
        "        out = self.to_out(out)\n",
        "\n",
        "        if self.shifted:\n",
        "            out = self.cyclic_back_shift(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9IN3MRG1odc"
      },
      "source": [
        "## Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GRLo_sCM1oGv"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
        "                                                                     heads=heads,\n",
        "                                                                     head_dim=head_dim,\n",
        "                                                                     shifted=shifted,\n",
        "                                                                     window_size=window_size,\n",
        "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
        "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention_block(x)\n",
        "        x = self.mlp_block(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y8qnQdhC6Hg"
      },
      "source": [
        "## Conv Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Branch 5 = 1x1->3x3->3x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, embed_dim, stride1=1):\n",
        "        super().__init__()\n",
        "        padding_3x3 = 1\n",
        "        padding_5x5 = 2\n",
        "\n",
        "        self.out_channels = embed_dim//3\n",
        "\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1, stride=stride1, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Branch 3: 1x1 Convolution followed by two 3x3 Convolutions (effective 5x5 receptive field)\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1), # First 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1), # Second 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        x = torch.cat((branch1x1, branch3x3, branch5x5), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Branch 5x5 = 1x1->5x5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, embed_dim, stride1=1):\n",
        "        super().__init__()\n",
        "        padding_3x3 = 1\n",
        "        padding_5x5 = 2\n",
        "\n",
        "        self.out_channels = embed_dim//3\n",
        "\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1, stride=stride1, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Branch 3: 1x1 Convolution followed by two 3x3 Convolutions (effective 5x5 receptive field)\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=5, padding=2, stride=stride1), # Second 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        x = torch.cat((branch1x1, branch3x3, branch5x5), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Branch 5 = 5x5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, embed_dim, stride1=1):\n",
        "        super().__init__()\n",
        "        padding_3x3 = 1\n",
        "        padding_5x5 = 2\n",
        "\n",
        "        self.out_channels = embed_dim//3\n",
        "\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1, stride=stride1, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Branch 3: 1x1 Convolution followed by two 3x3 Convolutions (effective 5x5 receptive field)\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=5, padding=2, stride=stride1), # Second 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        x = torch.cat((branch1x1, branch3x3, branch5x5), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "branch 3x3, 5x5, 7x7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, embed_dim, stride1=1):\n",
        "        super().__init__()\n",
        "        padding_3x3 = 1\n",
        "        padding_5x5 = 2\n",
        "\n",
        "        self.out_channels = embed_dim//3\n",
        "\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1, stride=stride1, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Branch 3: 1x1 Convolution followed by two 3x3 Convolutions (effective 5x5 receptive field)\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=5, padding=2, stride=stride1), # Second 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Branch 3: 1x1 Convolution followed by two 3x3 Convolutions (effective 5x5 receptive field)\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=7, padding=3, stride=stride1), # Second 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        branch7x7 = self.branch7x7(x)\n",
        "        x = torch.cat((branch3x3, branch5x5, branch7x7), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multigranularity CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiGranularitySummingBlock(nn.Module):\n",
        "    def __init__(self, in_channels, embed_dim, downscaling_factor=1, stride1=1):\n",
        "        super().__init__()\n",
        "        self.patch_merge = nn.Conv2d(in_channels=in_channels, out_channels=embed_dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_merge(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Branch 5x5 = 1x1->3x3->3x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiGranularitySummingBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, embed_dim: int, stride1: int = 1):\n",
        "        super().__init__()\n",
        "        padding_3x3 = 1\n",
        "        padding_5x5 = 2\n",
        "\n",
        "        self.out_channels = embed_dim//3\n",
        "\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1, stride=stride1, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Branch 3: 1x1 Convolution followed by two 3x3 Convolutions (effective 5x5 receptive field)\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1), # First 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1), # Second 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        x = torch.cat((branch1x1, branch3x3, branch5x5), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Branch5 = 1x1 -> 5x5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiGranularitySummingBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, embed_dim: int, stride1: int = 1):\n",
        "        super().__init__()\n",
        "        padding_3x3 = 1\n",
        "        padding_5x5 = 2\n",
        "\n",
        "        self.out_channels = embed_dim//3\n",
        "\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1, stride=stride1, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Branch 3: 1x1 Convolution followed by two 3x3 Convolutions (effective 5x5 receptive field)\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=5, padding=2), # First 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        x = torch.cat((branch1x1, branch3x3, branch5x5), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Branch 5 = 5x5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiGranularitySummingBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, embed_dim: int, stride1: int = 1):\n",
        "        super().__init__()\n",
        "        padding_3x3 = 1\n",
        "        padding_5x5 = 2\n",
        "\n",
        "        self.out_channels = embed_dim//3\n",
        "\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1, stride=stride1, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=3, stride=stride1, padding=padding_3x3, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=5, stride=stride1, padding=padding_5x5, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        x = torch.cat((branch1x1, branch3x3, branch5x5), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conv = 3x3, 5x5, 7x7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiGranularitySummingBlock(nn.Module):\n",
        "    def __init__(self, in_channels, embed_dim, stride1=1):\n",
        "        super().__init__()\n",
        "        padding_3x3 = 1\n",
        "        padding_5x5 = 2\n",
        "\n",
        "        self.out_channels = embed_dim//3\n",
        "\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1, stride=stride1, bias=False),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1),\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Branch 3: 1x1 Convolution followed by two 3x3 Convolutions (effective 5x5 receptive field)\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1), # First 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1), # Second 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Branch 3: 1x1 Convolution followed by two 3x3 Convolutions (effective 5x5 receptive field)\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, self.out_channels, kernel_size=1), # 1x1 to potentially reduce channels\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1), # First 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1), # Second 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, padding=1, stride=stride1), # Second 3x3\n",
        "            nn.BatchNorm2d(self.out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "        branch5x5 = self.branch5x5(x)\n",
        "        branch7x7 = self.branch7x7(x)\n",
        "        x = torch.cat((branch3x3, branch5x5, branch7x7), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDWR1fCU1v87"
      },
      "source": [
        "## Patch Merging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xjf409VI1qxT"
      },
      "outputs": [],
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "        super().__init__()\n",
        "        self.downscaling_factor = downscaling_factor\n",
        "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
        "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        print(b,c,h,w)\n",
        "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
        "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dual Switch Just swap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DualSwitch_SwapOnly(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DualSwitch_SwapOnly, self).__init__()\n",
        "\n",
        "    def _switch_adjacent(self, input_tensor: torch.Tensor, dim: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Switches adjacent elements along a specified dimension.\n",
        "        If the dimension size is odd, the last element remains untouched.\n",
        "        Returns a new tensor.\n",
        "        \"\"\"\n",
        "        size = input_tensor.shape[dim]\n",
        "        output_tensor = input_tensor.clone() # Start with a copy of the input\n",
        "\n",
        "        # Determine the largest even size that can be fully swapped\n",
        "        swappable_size = (size // 2) * 2\n",
        "\n",
        "        if swappable_size > 0:\n",
        "            # Create slices for even and odd indices within the swappable part\n",
        "            slices_even_part = [slice(None)] * input_tensor.ndim\n",
        "            slices_odd_part = [slice(None)] * input_tensor.ndim\n",
        "            \n",
        "            slices_even_part[dim] = slice(0, swappable_size, 2)  # 0, 2, 4, ...\n",
        "            slices_odd_part[dim] = slice(1, swappable_size + 1, 2) # 1, 3, 5, ...\n",
        "\n",
        "            # Perform the swap on the output_tensor\n",
        "            output_tensor[slices_even_part] = input_tensor[slices_odd_part]\n",
        "            output_tensor[slices_odd_part] = input_tensor[slices_even_part]\n",
        "            \n",
        "        return output_tensor\n",
        "\n",
        "    def _switch_interlaced(self, input_tensor: torch.Tensor, dim: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Switches interlaced blocks of 2 elements along a specified dimension.\n",
        "        If the dimension size is not a multiple of 4, the trailing elements remain untouched.\n",
        "        Returns a new tensor.\n",
        "        \"\"\"\n",
        "        size = input_tensor.shape[dim]\n",
        "        \n",
        "        indices = torch.arange(size, device=input_tensor.device)\n",
        "        new_indices = indices.clone() # Initialize with identity permutation\n",
        "\n",
        "        # Determine the largest size that is a multiple of 4 and can be fully swapped\n",
        "        swappable_size = (size // 4) * 4\n",
        "\n",
        "        for i in range(0, swappable_size, 4):\n",
        "            # Swap blocks of 2: [i, i+1] goes to [i+2, i+3] positions\n",
        "            new_indices[i:i+2] = indices[i+2:i+4]\n",
        "            # And [i+2, i+3] goes to [i, i+1] positions\n",
        "            new_indices[i+2:i+4] = indices[i:i+2]\n",
        "        \n",
        "        # Apply the permutation using advanced indexing, which creates a new tensor\n",
        "        all_slices = [slice(None)] * input_tensor.ndim\n",
        "        all_slices[dim] = new_indices # Apply the reordered indices to the specified dimension\n",
        "        return input_tensor[all_slices]\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a sequence of column and row switching operations on the feature map.\n",
        "\n",
        "        Following the convention:\n",
        "        - Columns refer to the Height (H) dimension (dim=2).\n",
        "        - Rows refer to the Width (W) dimension (dim=3).\n",
        "        \n",
        "        The operations are:\n",
        "        1. Adjacent column switching (on H).\n",
        "        2. Adjacent row switching (on W).\n",
        "        3. Interlaced column switching (on H, swaps blocks of 2).\n",
        "        4. Interlaced row switching (on W, swaps blocks of 2).\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input feature map of shape (B, C, H, W).\n",
        "                              No internal dimension checks are performed;\n",
        "                              trailing elements in odd/non-multiple-of-4 dimensions\n",
        "                              will be left untouched by the respective operations.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The feature map after all switching operations.\n",
        "                          The output shape is identical to the input shape.\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Adjacent columns switch (Columns is H, so dim=2)\n",
        "        x = self._switch_adjacent(x, dim=2)\n",
        "        \n",
        "        # Step 2: Adjacent rows switch (Rows is W, so dim=3)\n",
        "        x = self._switch_adjacent(x, dim=3)\n",
        "        \n",
        "        # Step 3: Interlaced columns switch (Columns is H, so dim=2)\n",
        "        x = self._switch_interlaced(x, dim=2)\n",
        "\n",
        "        # Step 4: Interlaced rows switch (Rows is W, so dim=3)\n",
        "        x = self._switch_interlaced(x, dim=3)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dual Switch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DualSwitching(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "        super().__init__()\n",
        "        self.downscaling_factor = downscaling_factor\n",
        "        self.multi_granularity_summing_block = MultiGranularitySummingBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        print(b,c,h,w)\n",
        "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
        "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
        "\n",
        "# -------------Dual Switching---------------------\n",
        "        # Swap Adjacent Column\n",
        "        print(\"\\n--- Performing Adjacent Column Swap (0<->1, 2<->3, ...) ---\")\n",
        "        if new_w < 2:\n",
        "            print(\"Not enough columns for adjacent column swap. Skipping.\")\n",
        "        else:\n",
        "            x_adj_col_swapped = torch.empty_like(x)\n",
        "            for j in range(0, new_w - 1, 2):\n",
        "                x_adj_col_swapped[:, :, j, :] = x[:, :, j+1, :]\n",
        "                x_adj_col_swapped[:, :, j+1, :] = x[:, :, j, :]\n",
        "            if new_w % 2 != 0:\n",
        "                x_adj_col_swapped[:, :, new_w - 1, :] = x[:, :, new_w - 1, :]\n",
        "            x = x_adj_col_swapped\n",
        "        print(f\"Shape after adjacent column swap: {x.shape}\")\n",
        "\n",
        "        # Swap Ajacent Rows\n",
        "        print(\"\\n--- Performing Adjacent Row Swap (0<->1, 2<->3, ...) ---\")\n",
        "        if new_h < 2:\n",
        "            print(\"Not enough rows for adjacent row swap. Skipping.\")\n",
        "        else:\n",
        "            x_adj_row_swapped = torch.empty_like(x)\n",
        "            for i in range(0, new_h - 1, 2):\n",
        "                x_adj_row_swapped[:, i, :, :] = x[:, i+1, :, :]\n",
        "                x_adj_row_swapped[:, i+1, :, :] = x[:, i, :, :]\n",
        "            if new_h % 2 != 0:\n",
        "                x_adj_row_swapped[:, new_h - 1, :, :] = x[:, new_h - 1, :, :]\n",
        "            x = x_adj_row_swapped\n",
        "        print(f\"Shape after adjacent row swap: {x.shape}\")\n",
        "\n",
        "        #Swap Interlaced Column\n",
        "        print(\"\\n--- Performing Interlaced Column Swap (1<->2, 3<->4, ...) ---\")\n",
        "\n",
        "        if new_w < 3:\n",
        "            print(\"Not enough columns for interlaced column swap (need at least 3). Skipping.\")\n",
        "        else:\n",
        "            x_int_col_swapped = torch.empty_like(x)\n",
        "\n",
        "            x_int_col_swapped[:, :, 0, :] = x[:, :, 0, :].clone()\n",
        "\n",
        "            for j_start in range(1, new_w - 1, 2):\n",
        "                x_int_col_swapped[:, :, j_start, :] = x[:, :, j_start + 1, :]\n",
        "                x_int_col_swapped[:, :, j_start + 1, :] = x[:, :, j_start, :]\n",
        "\n",
        "            x_int_col_swapped = x.clone()\n",
        "\n",
        "            for j_start in range(1, new_w - 1, 2):\n",
        "                temp_col_j = x[:, :, j_start, :].clone() #\n",
        "                x_int_col_swapped[:, :, j_start, :] = x[:, :, j_start + 1, :]\n",
        "                x_int_col_swapped[:, :, j_start + 1, :] = temp_col_j\n",
        "            x = x_int_col_swapped\n",
        "        print(f\"Shape after interlaced column swap: {x.shape}\")\n",
        "\n",
        "        #Swap Interlaced Rows\n",
        "        print(\"\\n--- Performing Interlaced Row Swap (1<->2, 3<->4, ...) ---\")\n",
        "        if new_h < 3: # Need at least 3 rows (indices 0, 1, 2)\n",
        "            print(\"Not enough rows for interlaced row swap (need at least 3). Skipping.\")\n",
        "        else:\n",
        "            x_int_row_swapped = x.clone() # Start with the current state of x\n",
        "\n",
        "            for i_start in range(1, new_h - 1, 2): # Loop for 1, 3, 5, ...\n",
        "                temp_row_i = x[:, i_start, :, :].clone() # Backup original row i_start\n",
        "                x_int_row_swapped[:, i_start, :, :] = x[:, i_start + 1, :, :]\n",
        "                x_int_row_swapped[:, i_start + 1, :, :] = temp_row_i\n",
        "            x = x_int_row_swapped\n",
        "        print(f\"Shape after interlaced row swap: {x.shape}\")\n",
        "\n",
        "\n",
        "# -----------------End Dual Switching-----------\n",
        "\n",
        "        x = self.linear(x)\n",
        "        print(f\"Final output shape: {x.shape}\")\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCpSgbWk4eFw"
      },
      "source": [
        "## Stage Module (del module 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StageModule(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, TB_layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
        "                                            downscaling_factor=downscaling_factor)\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "\n",
        "        for _ in range(TB_layers):\n",
        "            self.layers.append(\n",
        "                TransformerBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "               )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_partition(x)\n",
        "        \n",
        "        for regular_block in self.layers:\n",
        "            x = regular_block(x)\n",
        "        return x.permute(0, 3, 1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKo4hZItJwjZ"
      },
      "source": [
        "## Hyneter Module (del module 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyneter_no_CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HyneterModule(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, Conv_layers,TB_layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        self.mg = MultiGranularitySummingBlock(in_channels=in_channels, embed_dim=hidden_dimension, stride1=1)   \n",
        "\n",
        "        self.TB_layers = nn.ModuleList([])\n",
        "        for _ in range(TB_layers):\n",
        "            self.TB_layers.append(\n",
        "                TransformerBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "               )          \n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"\\nHyneterModule: HNB: Patch -> TB\")\n",
        "        print(\"HyneterModule no CNN forward pass\")\n",
        "        \n",
        "        print(f\"\\nInput shape before Multigranularity CNN: {x.shape}\") # Batch size, Channels, Height, Width\n",
        "        x = self.mg(x)\n",
        "        print(f\"Input shape after Multigranularity CNN: {x.shape}\") # Batch size, Channels, Height, Width\n",
        "\n",
        "        x_tb_path = x\n",
        "\n",
        "        x_tb_path = x_tb_path.permute(0, 2, 3, 1) # Change to (batch_size, Height, Width, Channels)\n",
        "\n",
        "        print(\"X(TB) shape before Transformer Blocks:\", x_tb_path.shape) # Batch size, Height, Width, Channels\n",
        "        for block in self.TB_layers:\n",
        "            x_tb_path = block(x_tb_path)\n",
        "        print(f\"Input shape after Transformer Blocks: {x_tb_path.shape}\") # Batch size, Height, Width, Channels\n",
        "        x_tb_path = x_tb_path.permute(0, 3, 1, 2) # Change to (batch_size, channels, height, width)\n",
        "        print(\"X(TB) shape:\", x_tb_path.shape) # B, C, H, W\n",
        "\n",
        "        return x_tb_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HyneterModule_DualSwitch(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, Conv_layers,TB_layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        self.mg = MultiGranularitySummingBlock(in_channels=in_channels, embed_dim=hidden_dimension, stride1=1)   \n",
        "\n",
        "        self.DualSwitching = DualSwitch_SwapOnly()\n",
        "\n",
        "        self.TB_layers = nn.ModuleList([])\n",
        "        for _ in range(TB_layers):\n",
        "            self.TB_layers.append(\n",
        "                TransformerBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "               )          \n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"\\nHyneterModule: HNB: Patch -> TB\")\n",
        "        print(\"HyneterModule no CNN forward pass\")\n",
        "        \n",
        "        print(f\"\\nInput shape before Multigranularity CNN: {x.shape}\") # Batch size, Channels, Height, Width\n",
        "        x = self.mg(x)\n",
        "        print(f\"Input shape after Multigranularity CNN: {x.shape}\") # Batch size, Channels, Height, Width\n",
        "\n",
        "        x = self.DualSwitching(x)\n",
        "\n",
        "        x_tb_path = x\n",
        "\n",
        "        x_tb_path = x_tb_path.permute(0, 2, 3, 1) # Change to (batch_size, Height, Width, Channels)\n",
        "\n",
        "        print(\"X(TB) shape before Transformer Blocks:\", x_tb_path.shape) # Batch size, Height, Width, Channels\n",
        "        for block in self.TB_layers:\n",
        "            x_tb_path = block(x_tb_path)\n",
        "        print(f\"Input shape after Transformer Blocks: {x_tb_path.shape}\") # Batch size, Height, Width, Channels\n",
        "        x_tb_path = x_tb_path.permute(0, 3, 1, 2) # Change to (batch_size, channels, height, width)\n",
        "        print(\"X(TB) shape:\", x_tb_path.shape) # B, C, H, W\n",
        "\n",
        "        return x_tb_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HyneterModule_noHNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XdDSM6r5JySh"
      },
      "outputs": [],
      "source": [
        "class HyneterModule_noHNB(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, Conv_layers,TB_layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
        "                                            downscaling_factor=downscaling_factor)\n",
        "\n",
        "        self.Conv_layers = nn.ModuleList([])\n",
        "        for _ in range(Conv_layers):\n",
        "            self.Conv_layers.append(\n",
        "                CNN(in_channels=in_channels,embed_dim=hidden_dimension)\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        self.TB_layers = nn.ModuleList([])\n",
        "        for _ in range(TB_layers):\n",
        "            self.TB_layers.append(\n",
        "                TransformerBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "               )\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"HyneterModule_noHNB forward pass\")\n",
        "\n",
        "        print(f\"Input shape before Patch Partition: {x.shape}\")\n",
        "        x = self.patch_partition(x)\n",
        "        print(f\"Input shape after Patch Partition: {x.shape}\")\n",
        "        \n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        print(\"CNN input shape:\", x.shape)\n",
        "        for block in self.Conv_layers:\n",
        "            x = block(x)\n",
        "        print(f\"CNN Output shape: {x.shape}\")\n",
        "        x = x.permute(0, 2, 3, 1)  # Change to (batch_size, height, width, channels)\n",
        "\n",
        "\n",
        "        print(f\"Input shape after Conv layers: {x.shape}\")\n",
        "        for block in self.TB_layers:\n",
        "            x = block(x)\n",
        "        print(f\"Input shape after Transformer Blocks: {x.shape}\")\n",
        "        return x.permute(0, 3, 1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HyneterModule(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, Conv_layers,TB_layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        self.mg = MultiGranularitySummingBlock(in_channels=in_channels, embed_dim=hidden_dimension, stride1=1)\n",
        "\n",
        "        self.Conv_layers = nn.ModuleList([])\n",
        "        for _ in range(Conv_layers):\n",
        "            self.Conv_layers.append(\n",
        "                CNN(in_channels=hidden_dimension,embed_dim=hidden_dimension)\n",
        "            )\n",
        "\n",
        "        \n",
        "\n",
        "        self.TB_layers = nn.ModuleList([])\n",
        "        for _ in range(TB_layers):\n",
        "            self.TB_layers.append(\n",
        "                TransformerBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "               )          \n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"\\nHyneterModule: HNB: Patch -> Conv -> TB\")\n",
        "        print(\"HyneterModule forward pass\")\n",
        "        \n",
        "        \n",
        "        print(f\"\\nInput shape before Multigranularity CNN: {x.shape}\") # Batch size, Channels, Height, Width\n",
        "        x = self.mg(x)\n",
        "        print(f\"Input shape after Multigranularity CNN: {x.shape}\") # Batch size, Channels, Height, Width\n",
        "\n",
        "        x_conv_path = x.clone()\n",
        "        x_tb_path = x\n",
        "\n",
        "        x_tb_path = x_tb_path.permute(0, 2, 3, 1) # Change to (batch_size, Height, Width, Channels)\n",
        "\n",
        "        print(\"X(CNN) shape before Conv layers:\", x_conv_path.shape) # Batch size, Channels, Height, Width\n",
        "        for block in self.Conv_layers:\n",
        "            x_conv_path = block(x_conv_path)\n",
        "        print(f\"X(CNN) shape after Conv layers: {x_conv_path.shape}\") # Batch size, Channels, Height, Width\n",
        "\n",
        "\n",
        "        print(\"X(TB) shape before Transformer Blocks:\", x_tb_path.shape) # Batch size, Height, Width, Channels\n",
        "        for block in self.TB_layers:\n",
        "            x_tb_path = block(x_tb_path)\n",
        "        print(f\"Input shape after Transformer Blocks: {x_tb_path.shape}\") # Batch size, Height, Width, Channels\n",
        "        x_tb_path = x_tb_path.permute(0, 3, 1, 2) # Change to (batch_size, channels, height, width)\n",
        "\n",
        "\n",
        "        print(\"########### going to calculate Z ###########\")\n",
        "        print(\"X(CNN) shape:\", x_conv_path.shape) # B, C, H, W\n",
        "        print(\"X(TB) shape:\", x_tb_path.shape) # B, C, H, W\n",
        "\n",
        "\n",
        "        Z = x_conv_path * x_tb_path\n",
        "        print(f\"Z shape\", Z.shape) # B, C, H, W\n",
        "        print(\"Z before tanh:\", Z)\n",
        "        Z = torch.tanh(Z)\n",
        "        print(\"Z after tanh:\", Z)\n",
        "        print(\"Z is tanh(Dot product between x and S):\", Z.shape) # B, C, H, W\n",
        "\n",
        "        output = x_tb_path+Z\n",
        "        print(\"output shape after adding Z:\", x.shape) # B, C, H, W\n",
        "        print(\"output value:\", output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HyneterModule_DualSwitch(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, Conv_layers,TB_layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        self.mg = MultiGranularitySummingBlock(in_channels=in_channels, embed_dim=hidden_dimension, stride1=1)\n",
        "\n",
        "        self.Conv_layers = nn.ModuleList([])\n",
        "        for _ in range(Conv_layers):\n",
        "            self.Conv_layers.append(\n",
        "                CNN(in_channels=hidden_dimension,embed_dim=hidden_dimension)\n",
        "            )\n",
        "\n",
        "\n",
        "        self.DualSwitching = DualSwitch_SwapOnly()\n",
        "\n",
        "        self.TB_layers = nn.ModuleList([])\n",
        "        for _ in range(TB_layers):\n",
        "            self.TB_layers.append(\n",
        "                TransformerBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "               )\n",
        "            \n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"\\nHyneterModule: HNB: Patch -> Conv -> TB\")\n",
        "        print(\"HyneterModule forward pass\")\n",
        "        \n",
        "        \n",
        "        print(f\"\\nInput shape before Multigranularity CNN: {x.shape}\") # Batch size, Channels, Height, Width\n",
        "        x = self.mg(x)\n",
        "        print(f\"Input shape after Multigranularity CNN: {x.shape}\") # Batch size, Channels, Height, Width\n",
        "\n",
        "        x_conv_path = x.clone()\n",
        "        x_tb_path = x\n",
        "\n",
        "\n",
        "        print(\"X(CNN) shape before Conv layers:\", x_conv_path.shape) # Batch size, Channels, Height, Width\n",
        "        for block in self.Conv_layers:\n",
        "            x_conv_path = block(x_conv_path)\n",
        "        print(f\"X(CNN) shape after Conv layers: {x_conv_path.shape}\") # Batch size, Channels, Height, Width\n",
        "    \n",
        "        x_tb_path = self.DualSwitching(x_tb_path)\n",
        "\n",
        "        x_tb_path = x_tb_path.permute(0, 2, 3, 1) # Change to (batch_size, Height, Width, Channels)\n",
        "        print(\"X(TB) shape before Transformer Blocks:\", x_tb_path.shape) # Batch size, Height, Width, Channels\n",
        "        for block in self.TB_layers:\n",
        "            x_tb_path = block(x_tb_path)\n",
        "        print(f\"Input shape after Transformer Blocks: {x_tb_path.shape}\") # Batch size, Height, Width, Channels\n",
        "        x_tb_path = x_tb_path.permute(0, 3, 1, 2) # Change to (batch_size, channels, height, width)\n",
        "\n",
        "\n",
        "        print(\"########### going to calculate Z ###########\")\n",
        "        print(\"X(CNN) shape:\", x_conv_path.shape) # B, C, H, W\n",
        "        print(\"X(TB) shape:\", x_tb_path.shape) # B, C, H, W\n",
        "\n",
        "\n",
        "        Z = x_conv_path * x_tb_path\n",
        "        print(f\"Z shape\", Z.shape) # B, C, H, W\n",
        "        print(\"Z before tanh:\", Z)\n",
        "        Z = torch.tanh(Z)\n",
        "        print(\"Z after tanh:\", Z)\n",
        "        print(\"Z is tanh(Dot product between x and S):\", Z.shape) # B, C, H, W\n",
        "\n",
        "        output = x_tb_path+Z\n",
        "        print(\"output shape after adding Z:\", x.shape) # B, C, H, W\n",
        "        print(\"output value:\", output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glO5-58J4gsq"
      },
      "source": [
        "## Stage Module (Dual Switch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vG7jypvP5jq-"
      },
      "outputs": [],
      "source": [
        "class StageModule(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, TB_layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        assert TB_layers % 2 == 0, 'Stage TB_layers need to be divisible by 2 for regular and shifted block.'\n",
        "\n",
        "        self.patch_partition = DualSwitching(in_channels=in_channels, out_channels=hidden_dimension,\n",
        "                                            downscaling_factor=downscaling_factor)\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "\n",
        "        for _ in range(TB_layers):\n",
        "            self.layers.append(\n",
        "                TransformerBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "               )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_partition(x)\n",
        "        \n",
        "        for regular_block in self.layers:\n",
        "            x = regular_block(x)\n",
        "        return x.permute(0, 3, 1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gszhmDzs2IRu"
      },
      "source": [
        "## Swin Transformer (No FPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "H71GruZ02Eij"
      },
      "outputs": [],
      "source": [
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, TB_layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.out_channels = hidden_dim * 8\n",
        "\n",
        "\n",
        "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, TB_layers=TB_layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, TB_layers=TB_layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, TB_layers=TB_layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, TB_layers=TB_layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.stage1(img)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = x.mean(dim=[2, 3])\n",
        "        return self.mlp_head(x)\n",
        "\n",
        "\n",
        "def swin_t(hidden_dim=96, TB_layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def swin_s(hidden_dim=96, TB_layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def swin_b(hidden_dim=128, TB_layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def swin_l(hidden_dim=192, TB_layers=(2, 2, 18, 2), heads=(6, 12, 24, 48), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Swin for FPN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class SwinForFPN(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, TB_layers, heads, channels=3, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.out_channels = hidden_dim * 8\n",
        "\n",
        "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, TB_layers=TB_layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, TB_layers=TB_layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, TB_layers=TB_layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, TB_layers=TB_layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        # c2 = self.stage1(img)\n",
        "        # c3 = self.stage2(x)\n",
        "        # c4 = self.stage3(x)\n",
        "        # c5 = self.stage4(x)\n",
        "        # # Return the feature maps for FPN\n",
        "        # return [c2, c3, c4, c5]\n",
        "\n",
        "        # for Use with Mask R-CNN from torchvision.models.detection\n",
        "        out = OrderedDict()\n",
        "        print(f\"Input image shape: {img.shape}\")\n",
        "        c2 = self.stage1(img)\n",
        "        out['0'] = c2\n",
        "        print(f\"Output of stage1 (c2) shape: {c2.shape}\")\n",
        "        c3 = self.stage2(c2)\n",
        "        out['1'] = c3\n",
        "        print(f\"Output of stage2 (c3) shape: {c3.shape}\")\n",
        "        c4 = self.stage3(c3)\n",
        "        out['2'] = c4\n",
        "        print(f\"Output of stage3 (c4) shape: {c4.shape}\")\n",
        "        c5 = self.stage4(c4)\n",
        "        out['3'] = c5\n",
        "        print(f\"Output of stage4 (c5) shape: {c5.shape}\")\n",
        "        # Return the feature maps for FPN\n",
        "        return out\n",
        "        \n",
        "\n",
        "\n",
        "def swin_t_fpn(hidden_dim=96, TB_layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return SwinForFPN(hidden_dim=hidden_dim, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "def swin_s_fpn(hidden_dim=96, TB_layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return SwinForFPN(hidden_dim=hidden_dim, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "def swin_b_fpn(hidden_dim=128, TB_layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
        "    return SwinForFPN(hidden_dim=hidden_dim, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "def swin_l_fpn(hidden_dim=192, TB_layers=(2, 2, 18, 2), heads=(6, 12, 24, 48), **kwargs):\n",
        "    return SwinForFPN(hidden_dim=hidden_dim, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Swin + FPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.ops import FeaturePyramidNetwork, MultiScaleRoIAlign\n",
        "\n",
        "\n",
        "class SwinFPNBackbone(nn.Module):\n",
        "    def __init__(self, backbone, hidden_dim=96,):\n",
        "        super().__init__()\n",
        "        self.swin_backbone = backbone\n",
        "        \n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=[hidden_dim, hidden_dim * 2, hidden_dim * 4, hidden_dim * 8],\n",
        "            out_channels=256,\n",
        "        )\n",
        "\n",
        "        self.out_channels = 256\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.swin_backbone(x)\n",
        "\n",
        "        fpn_features = self.fpn(features)\n",
        "\n",
        "        return fpn_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mask R-CNN + Swin Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "\n",
        "\n",
        "def get_swin_mask_rcnn_model(num_class=91):\n",
        "    backbone = SwinFPNBackbone(swin_t_fpn())\n",
        "\n",
        "    fpn_output_keys = ['0', '1', '2', '3']  # Corresponds to P2, P3, P4, P5 in FPN\n",
        "    NUM_FPN_OUTPUT_LEVELS = len(fpn_output_keys)\n",
        "\n",
        "    anchor_sizes = (\n",
        "        (32,),\n",
        "        (64,),\n",
        "        (128,),\n",
        "        (256,),\n",
        "        (512,)\n",
        "        )\n",
        "    aspect_ratios = ((0.5, 1.0, 2.0),) * NUM_FPN_OUTPUT_LEVELS\n",
        "    anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
        "\n",
        "\n",
        "    # ROI Poolers must use the FPN output channels (out_channels, which is 256)\n",
        "    # featmap_names should match the FPN's output names (0, 1, 2, 3 for P2, P3, P4, P5)\n",
        "    box_roi_pool = MultiScaleRoIAlign(featmap_names=fpn_output_keys, output_size=7, sampling_ratio=2)\n",
        "    mask_roi_pool = MultiScaleRoIAlign(featmap_names=fpn_output_keys, output_size=14, sampling_ratio=2)\n",
        "\n",
        "\n",
        "    model = MaskRCNN(\n",
        "        backbone=backbone,\n",
        "        num_classes=num_class,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=box_roi_pool,\n",
        "        mask_roi_pool=mask_roi_pool,\n",
        "        min_size=224,\n",
        "        max_size=224\n",
        "    )\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jSpYi-zJ7Hf"
      },
      "source": [
        "# Hyneter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umVjt_J4J2h7"
      },
      "outputs": [],
      "source": [
        "class Hyneter(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, Conv_layers, TB_layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.out_channels = hidden_dim * 8\n",
        "\n",
        "        self.stage1 = HyneterModule(in_channels=channels, hidden_dimension=hidden_dim, Conv_layers=Conv_layers[0], TB_layers=TB_layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage2 = HyneterModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, Conv_layers=Conv_layers[1], TB_layers=TB_layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage3 = HyneterModule_DualSwitch(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, Conv_layers=Conv_layers[2], TB_layers=TB_layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage4 = HyneterModule_DualSwitch(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, Conv_layers=Conv_layers[3], TB_layers=TB_layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.stage1(img)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = x.mean(dim=[2, 3])\n",
        "        print(f\"Final output shape before MLP head: {x.shape}\")\n",
        "        return self.mlp_head(x)\n",
        "\n",
        "\n",
        "\n",
        "def hyneter_base(hidden_dim=96, Conv_layers=(2, 2, 2, 2), TB_layers=(2, 2, 2, 2), heads=(4, 8, 16, 32), **kwargs):\n",
        "    return Hyneter(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_plus(hidden_dim=96, Conv_layers=(2, 2, 3, 2), TB_layers=(2, 2, 6, 2), heads=(4, 8, 16, 32), **kwargs):\n",
        "    return Hyneter(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_max(hidden_dim=96, Conv_layers=(2, 2, 6, 2), TB_layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
        "    return Hyneter(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyneter with FPN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyneter No CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class HyneterForFPN(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, Conv_layers, TB_layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.out_channels = hidden_dim * 8\n",
        "\n",
        "        self.stage1 = HyneterModule_noCNN(in_channels=channels, hidden_dimension=hidden_dim, Conv_layers=Conv_layers[0], TB_layers=TB_layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage2 = HyneterModule_noCNN(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, Conv_layers=Conv_layers[1], TB_layers=TB_layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage3 = HyneterModule_noCNN(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, Conv_layers=Conv_layers[2], TB_layers=TB_layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage4 = HyneterModule_noCNN(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, Conv_layers=Conv_layers[3], TB_layers=TB_layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        \n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        " \n",
        "        out = OrderedDict()\n",
        "        print(f\"Input image shape: {img.shape}\")\n",
        "        c2 = self.stage1(img)\n",
        "        out['0'] = c2\n",
        "        print(f\"#Output of stage1 (c2) shape: {c2.shape}\")\n",
        "        c3 = self.stage2(c2)\n",
        "        out['1'] = c3\n",
        "        print(f\"#Output of stage2 (c3) shape: {c3.shape}\")\n",
        "        c4 = self.stage3(c3)\n",
        "        out['2'] = c4\n",
        "        print(f\"#Output of stage3 (c4) shape: {c4.shape}\")\n",
        "        c5 = self.stage4(c4)\n",
        "        out['3'] = c5\n",
        "        print(f\"#Output of stage4 (c5) shape: {c5.shape}\")\n",
        "        # Return the feature maps for FPN\n",
        "        return out\n",
        "        \n",
        "\n",
        "\n",
        "def hyneter_base_fpn(hidden_dim=96, Conv_layers=(2, 2, 2, 2), TB_layers=(2, 2, 2, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_plus_fpn(hidden_dim=96, Conv_layers=(2, 2, 3, 2), TB_layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_max_fpn(hidden_dim=96, Conv_layers=(2, 2, 6, 2), TB_layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "def hyneter_swin_size_fpn(hidden_dim=128, Conv_layers=(2, 2, 6, 2), TB_layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyneter "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class HyneterForFPN(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, Conv_layers, TB_layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.out_channels = hidden_dim * 8\n",
        "\n",
        "        self.stage1 = HyneterModule_noHNB(in_channels=channels, hidden_dimension=hidden_dim, Conv_layers=Conv_layers[0], TB_layers=TB_layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage2 = HyneterModule_noHNB(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, Conv_layers=Conv_layers[1], TB_layers=TB_layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage3 = HyneterModule_noHNB(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, Conv_layers=Conv_layers[2], TB_layers=TB_layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage4 = HyneterModule_noHNB(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, Conv_layers=Conv_layers[3], TB_layers=TB_layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        " \n",
        "        out = OrderedDict()\n",
        "        print(f\"Input image shape: {img.shape}\")\n",
        "        c2 = self.stage1(img)\n",
        "        out['0'] = c2\n",
        "        print(f\"#Output of stage1 (c2) shape: {c2.shape}\")\n",
        "        c3 = self.stage2(c2)\n",
        "        out['1'] = c3\n",
        "        print(f\"#Output of stage2 (c3) shape: {c3.shape}\")\n",
        "        c4 = self.stage3(c3)\n",
        "        out['2'] = c4\n",
        "        print(f\"#Output of stage3 (c4) shape: {c4.shape}\")\n",
        "        c5 = self.stage4(c4)\n",
        "        out['3'] = c5\n",
        "        print(f\"#Output of stage4 (c5) shape: {c5.shape}\")\n",
        "        # Return the feature maps for FPN\n",
        "        return out\n",
        "        \n",
        "\n",
        "\n",
        "def hyneter_base_fpn(hidden_dim=96, Conv_layers=(2, 2, 2, 2), TB_layers=(2, 2, 2, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_plus_fpn(hidden_dim=96, Conv_layers=(2, 2, 3, 2), TB_layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_max_fpn(hidden_dim=96, Conv_layers=(2, 2, 6, 2), TB_layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyneter with HNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class HyneterForFPN(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, Conv_layers, TB_layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.out_channels = hidden_dim * 8\n",
        "\n",
        "        self.stage1 = HyneterModule(in_channels=channels, hidden_dimension=hidden_dim, Conv_layers=Conv_layers[0], TB_layers=TB_layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage2 = HyneterModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, Conv_layers=Conv_layers[1], TB_layers=TB_layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage3 = HyneterModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, Conv_layers=Conv_layers[2], TB_layers=TB_layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage4 = HyneterModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, Conv_layers=Conv_layers[3], TB_layers=TB_layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        " \n",
        "        out = OrderedDict()\n",
        "        print(f\"#Input image shape: {img.shape}\")\n",
        "        c2 = self.stage1(img)\n",
        "        out['0'] = c2\n",
        "        print(f\"#Output of stage1 (c2) shape: {c2.shape}\")\n",
        "        c3 = self.stage2(c2)\n",
        "        out['1'] = c3\n",
        "        print(f\"#Output of stage2 (c3) shape: {c3.shape}\")\n",
        "        c4 = self.stage3(c3)\n",
        "        out['2'] = c4\n",
        "        print(f\"#Output of stage3 (c4) shape: {c4.shape}\")\n",
        "        c5 = self.stage4(c4)\n",
        "        out['3'] = c5\n",
        "        print(f\"#Output of stage4 (c5) shape: {c5.shape}\")\n",
        "        # Return the feature maps for FPN\n",
        "        return out\n",
        "        \n",
        "\n",
        "\n",
        "def hyneter_base_fpn(hidden_dim=96, Conv_layers=(2, 2, 2, 2), TB_layers=(2, 2, 2, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_plus_fpn(hidden_dim=96, Conv_layers=(2, 2, 3, 2), TB_layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_max_fpn(hidden_dim=128, Conv_layers=(2, 2, 6, 2), TB_layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyneter Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class HyneterForFPN(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, Conv_layers, TB_layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.out_channels = hidden_dim * 8\n",
        "\n",
        "        self.stage1 = HyneterModule(in_channels=channels, hidden_dimension=hidden_dim, Conv_layers=Conv_layers[0], TB_layers=TB_layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage2 = HyneterModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, Conv_layers=Conv_layers[1], TB_layers=TB_layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage3 = HyneterModule_DualSwitch(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, Conv_layers=Conv_layers[2], TB_layers=TB_layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage4 = HyneterModule_DualSwitch(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, Conv_layers=Conv_layers[3], TB_layers=TB_layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        \n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.stage1(img)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = x.mean(dim=[2, 3])\n",
        "        x = self.mlp_head(x)\n",
        "        print(f\"Output shape after Hyneter forward pass: {x.shape}\")\n",
        "        return x\n",
        "        \n",
        "\n",
        "\n",
        "def hyneter_base(hidden_dim=96, Conv_layers=(2, 2, 2, 2), TB_layers=(2, 2, 2, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return Hyneter(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_plus(hidden_dim=96, Conv_layers=(2, 2, 3, 2), TB_layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return Hyneter(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_max(hidden_dim=128, Conv_layers=(2, 2, 6, 2), TB_layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
        "    return Hyneter(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyneter With HNB + DS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.transforms import functional as F\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class HyneterForFPN(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, Conv_layers, TB_layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.out_channels = hidden_dim * 8\n",
        "\n",
        "        self.stage1 = HyneterModule(in_channels=channels, hidden_dimension=hidden_dim, Conv_layers=Conv_layers[0], TB_layers=TB_layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage2 = HyneterModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, Conv_layers=Conv_layers[1], TB_layers=TB_layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage3 = HyneterModule_DualSwitch(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, Conv_layers=Conv_layers[2], TB_layers=TB_layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage4 = HyneterModule_DualSwitch(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, Conv_layers=Conv_layers[3], TB_layers=TB_layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        \n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        " \n",
        "        out = OrderedDict()\n",
        "        print(\"HyneterForFPN forward pass\")\n",
        "        print(f\"#Input image shape: {img.shape}\")\n",
        "        c2 = self.stage1(img)\n",
        "        out['0'] = c2\n",
        "        print(f\"#Output of stage1 (c2) shape: {c2.shape}\")\n",
        "        c3 = self.stage2(c2)\n",
        "        out['1'] = c3\n",
        "        print(f\"#Output of stage2 (c3) shape: {c3.shape}\")\n",
        "        c4 = self.stage3(c3)\n",
        "        out['2'] = c4\n",
        "        print(f\"#Output of stage3 (c4) shape: {c4.shape}\")\n",
        "        c5 = self.stage4(c4)\n",
        "        out['3'] = c5\n",
        "        print(f\"#Output of stage4 (c5) shape: {c5.shape}\")\n",
        "        # Return the feature maps for FPN\n",
        "        return out\n",
        "        \n",
        "\n",
        "\n",
        "def hyneter_base_fpn(hidden_dim=96, Conv_layers=(2, 2, 2, 2), TB_layers=(2, 2, 2, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_plus_fpn(hidden_dim=96, Conv_layers=(2, 2, 3, 2), TB_layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n",
        "\n",
        "\n",
        "def hyneter_max_fpn(hidden_dim=128, Conv_layers=(2, 2, 6, 2), TB_layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
        "    return HyneterForFPN(hidden_dim=hidden_dim, Conv_layers=Conv_layers, TB_layers=TB_layers, heads=heads, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyneter + FPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.ops import FeaturePyramidNetwork, MultiScaleRoIAlign\n",
        "\n",
        "\n",
        "class HyneterFPNBackbone(nn.Module):\n",
        "    def __init__(self, backbone, hidden_dim=96,):\n",
        "        super().__init__()\n",
        "        self.hyneter_backbone = backbone\n",
        "        \n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=[hidden_dim, hidden_dim * 2, hidden_dim * 4, hidden_dim * 8],\n",
        "            out_channels=256,\n",
        "        )\n",
        "\n",
        "        self.out_channels = 256\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.hyneter_backbone(x)\n",
        "        print(f\"Features from Hyneter Backbone: {features.keys()}\")\n",
        "        print(f\"Input image shape: {x.shape}\")\n",
        "\n",
        "        fpn_features = self.fpn(features)\n",
        "        print(f\"FPN Features: {fpn_features.keys()}\")\n",
        "        print(f\"FPN Features shape: {[fpn_features[k].shape for k in fpn_features.keys()]}\")\n",
        "\n",
        "        return fpn_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyneter + FPN + Mask R-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "MultiGranularitySummingBlock.__init__() got an unexpected keyword argument 'embed_dim'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[54], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiScaleRoIAlign\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_hyneter_mask_rcnn_model\u001b[39m(backbone \u001b[38;5;241m=\u001b[39m \u001b[43mhyneter_base_fpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,num_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m91\u001b[39m):\n\u001b[0;32m      6\u001b[0m     backbone \u001b[38;5;241m=\u001b[39m HyneterFPNBackbone(backbone)\n\u001b[0;32m      8\u001b[0m     fpn_output_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Corresponds to P2, P3, P4, P5 in FPN\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[51], line 61\u001b[0m, in \u001b[0;36mhyneter_base_fpn\u001b[1;34m(hidden_dim, Conv_layers, TB_layers, heads, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhyneter_base_fpn\u001b[39m(hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m, Conv_layers\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), TB_layers\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), heads\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m24\u001b[39m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HyneterForFPN(hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim, Conv_layers\u001b[38;5;241m=\u001b[39mConv_layers, TB_layers\u001b[38;5;241m=\u001b[39mTB_layers, heads\u001b[38;5;241m=\u001b[39mheads, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[51], line 19\u001b[0m, in \u001b[0;36mHyneterForFPN.__init__\u001b[1;34m(self, hidden_dim, Conv_layers, TB_layers, heads, channels, num_classes, head_dim, window_size, downscaling_factors, relative_pos_embedding)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels \u001b[38;5;241m=\u001b[39m hidden_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage1 \u001b[38;5;241m=\u001b[39m \u001b[43mHyneterModule\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConv_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConv_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTB_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTB_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdownscaling_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownscaling_factors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage2 \u001b[38;5;241m=\u001b[39m HyneterModule(in_channels\u001b[38;5;241m=\u001b[39mhidden_dim, hidden_dimension\u001b[38;5;241m=\u001b[39mhidden_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, Conv_layers\u001b[38;5;241m=\u001b[39mConv_layers[\u001b[38;5;241m1\u001b[39m], TB_layers\u001b[38;5;241m=\u001b[39mTB_layers[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     23\u001b[0m                           downscaling_factor\u001b[38;5;241m=\u001b[39mdownscaling_factors[\u001b[38;5;241m1\u001b[39m], num_heads\u001b[38;5;241m=\u001b[39mheads[\u001b[38;5;241m1\u001b[39m], head_dim\u001b[38;5;241m=\u001b[39mhead_dim,\n\u001b[0;32m     24\u001b[0m                           window_size\u001b[38;5;241m=\u001b[39mwindow_size, relative_pos_embedding\u001b[38;5;241m=\u001b[39mrelative_pos_embedding)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage3 \u001b[38;5;241m=\u001b[39m HyneterModule_DualSwitch(in_channels\u001b[38;5;241m=\u001b[39mhidden_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, hidden_dimension\u001b[38;5;241m=\u001b[39mhidden_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, Conv_layers\u001b[38;5;241m=\u001b[39mConv_layers[\u001b[38;5;241m2\u001b[39m], TB_layers\u001b[38;5;241m=\u001b[39mTB_layers[\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m     26\u001b[0m                           downscaling_factor\u001b[38;5;241m=\u001b[39mdownscaling_factors[\u001b[38;5;241m2\u001b[39m], num_heads\u001b[38;5;241m=\u001b[39mheads[\u001b[38;5;241m2\u001b[39m], head_dim\u001b[38;5;241m=\u001b[39mhead_dim,\n\u001b[0;32m     27\u001b[0m                           window_size\u001b[38;5;241m=\u001b[39mwindow_size, relative_pos_embedding\u001b[38;5;241m=\u001b[39mrelative_pos_embedding)\n",
            "Cell \u001b[1;32mIn[41], line 5\u001b[0m, in \u001b[0;36mHyneterModule.__init__\u001b[1;34m(self, in_channels, hidden_dimension, Conv_layers, TB_layers, downscaling_factor, num_heads, head_dim, window_size, relative_pos_embedding)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels, hidden_dimension, Conv_layers,TB_layers, downscaling_factor, num_heads, head_dim, window_size,\n\u001b[0;32m      3\u001b[0m              relative_pos_embedding):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmg \u001b[38;5;241m=\u001b[39m \u001b[43mMultiGranularitySummingBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dimension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m   \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDualSwitching \u001b[38;5;241m=\u001b[39m DualSwitch_SwapOnly()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTB_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([])\n",
            "\u001b[1;31mTypeError\u001b[0m: MultiGranularitySummingBlock.__init__() got an unexpected keyword argument 'embed_dim'"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "\n",
        "\n",
        "def get_hyneter_mask_rcnn_model(backbone = hyneter_base_fpn(),num_class=91):\n",
        "    backbone = HyneterFPNBackbone(backbone)\n",
        "\n",
        "    fpn_output_keys = ['0', '1', '2', '3']  # Corresponds to P2, P3, P4, P5 in FPN\n",
        "    NUM_FPN_OUTPUT_LEVELS = len(fpn_output_keys)\n",
        "\n",
        "    anchor_sizes = (\n",
        "        (32,),\n",
        "        (64,),\n",
        "        (128,),\n",
        "        (256,),\n",
        "        (512,)\n",
        "        )\n",
        "    aspect_ratios = ((0.5, 1.0, 2.0),) * NUM_FPN_OUTPUT_LEVELS\n",
        "    anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
        "\n",
        "\n",
        "    # ROI Poolers must use the FPN output channels (out_channels, which is 256)\n",
        "    # featmap_names should match the FPN's output names (0, 1, 2, 3 for P2, P3, P4, P5)\n",
        "    box_roi_pool = MultiScaleRoIAlign(featmap_names=fpn_output_keys, output_size=7, sampling_ratio=2)\n",
        "    mask_roi_pool = MultiScaleRoIAlign(featmap_names=fpn_output_keys, output_size=14, sampling_ratio=2)\n",
        "\n",
        "\n",
        "    model = MaskRCNN(\n",
        "        backbone=backbone,\n",
        "        num_classes=num_class,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=box_roi_pool,\n",
        "        mask_roi_pool=mask_roi_pool,\n",
        "        min_size=224,\n",
        "        max_size=224\n",
        "    )\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Model Size & Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 64.38 M\n",
            "Trainable parameters: 64.38 M\n",
            "+--------------------------------------------------------+------------+\n",
            "|                         Module                         | Parameters |\n",
            "+--------------------------------------------------------+------------+\n",
            "|              stage1.mg.patch_merge.weight              |    2592    |\n",
            "|   stage1.TB_layers.0.attention_block.fn.norm.weight    |     96     |\n",
            "|    stage1.TB_layers.0.attention_block.fn.norm.bias     |     96     |\n",
            "| stage1.TB_layers.0.attention_block.fn.fn.pos_embedding |    169     |\n",
            "| stage1.TB_layers.0.attention_block.fn.fn.to_qkv.weight |   27648    |\n",
            "| stage1.TB_layers.0.attention_block.fn.fn.to_out.weight |    9216    |\n",
            "|  stage1.TB_layers.0.attention_block.fn.fn.to_out.bias  |     96     |\n",
            "|      stage1.TB_layers.0.mlp_block.fn.norm.weight       |     96     |\n",
            "|       stage1.TB_layers.0.mlp_block.fn.norm.bias        |     96     |\n",
            "|    stage1.TB_layers.0.mlp_block.fn.fn.net.0.weight     |   36864    |\n",
            "|     stage1.TB_layers.0.mlp_block.fn.fn.net.0.bias      |    384     |\n",
            "|    stage1.TB_layers.0.mlp_block.fn.fn.net.2.weight     |   36864    |\n",
            "|     stage1.TB_layers.0.mlp_block.fn.fn.net.2.bias      |     96     |\n",
            "|   stage1.TB_layers.1.attention_block.fn.norm.weight    |     96     |\n",
            "|    stage1.TB_layers.1.attention_block.fn.norm.bias     |     96     |\n",
            "| stage1.TB_layers.1.attention_block.fn.fn.pos_embedding |    169     |\n",
            "| stage1.TB_layers.1.attention_block.fn.fn.to_qkv.weight |   27648    |\n",
            "| stage1.TB_layers.1.attention_block.fn.fn.to_out.weight |    9216    |\n",
            "|  stage1.TB_layers.1.attention_block.fn.fn.to_out.bias  |     96     |\n",
            "|      stage1.TB_layers.1.mlp_block.fn.norm.weight       |     96     |\n",
            "|       stage1.TB_layers.1.mlp_block.fn.norm.bias        |     96     |\n",
            "|    stage1.TB_layers.1.mlp_block.fn.fn.net.0.weight     |   36864    |\n",
            "|     stage1.TB_layers.1.mlp_block.fn.fn.net.0.bias      |    384     |\n",
            "|    stage1.TB_layers.1.mlp_block.fn.fn.net.2.weight     |   36864    |\n",
            "|     stage1.TB_layers.1.mlp_block.fn.fn.net.2.bias      |     96     |\n",
            "|              stage2.mg.patch_merge.weight              |   165888   |\n",
            "|   stage2.TB_layers.0.attention_block.fn.norm.weight    |    192     |\n",
            "|    stage2.TB_layers.0.attention_block.fn.norm.bias     |    192     |\n",
            "| stage2.TB_layers.0.attention_block.fn.fn.pos_embedding |    169     |\n",
            "| stage2.TB_layers.0.attention_block.fn.fn.to_qkv.weight |   110592   |\n",
            "| stage2.TB_layers.0.attention_block.fn.fn.to_out.weight |   36864    |\n",
            "|  stage2.TB_layers.0.attention_block.fn.fn.to_out.bias  |    192     |\n",
            "|      stage2.TB_layers.0.mlp_block.fn.norm.weight       |    192     |\n",
            "|       stage2.TB_layers.0.mlp_block.fn.norm.bias        |    192     |\n",
            "|    stage2.TB_layers.0.mlp_block.fn.fn.net.0.weight     |   147456   |\n",
            "|     stage2.TB_layers.0.mlp_block.fn.fn.net.0.bias      |    768     |\n",
            "|    stage2.TB_layers.0.mlp_block.fn.fn.net.2.weight     |   147456   |\n",
            "|     stage2.TB_layers.0.mlp_block.fn.fn.net.2.bias      |    192     |\n",
            "|   stage2.TB_layers.1.attention_block.fn.norm.weight    |    192     |\n",
            "|    stage2.TB_layers.1.attention_block.fn.norm.bias     |    192     |\n",
            "| stage2.TB_layers.1.attention_block.fn.fn.pos_embedding |    169     |\n",
            "| stage2.TB_layers.1.attention_block.fn.fn.to_qkv.weight |   110592   |\n",
            "| stage2.TB_layers.1.attention_block.fn.fn.to_out.weight |   36864    |\n",
            "|  stage2.TB_layers.1.attention_block.fn.fn.to_out.bias  |    192     |\n",
            "|      stage2.TB_layers.1.mlp_block.fn.norm.weight       |    192     |\n",
            "|       stage2.TB_layers.1.mlp_block.fn.norm.bias        |    192     |\n",
            "|    stage2.TB_layers.1.mlp_block.fn.fn.net.0.weight     |   147456   |\n",
            "|     stage2.TB_layers.1.mlp_block.fn.fn.net.0.bias      |    768     |\n",
            "|    stage2.TB_layers.1.mlp_block.fn.fn.net.2.weight     |   147456   |\n",
            "|     stage2.TB_layers.1.mlp_block.fn.fn.net.2.bias      |    192     |\n",
            "|              stage3.mg.patch_merge.weight              |   663552   |\n",
            "|        stage3.Conv_layers.0.branch1x1.0.weight         |   49152    |\n",
            "|        stage3.Conv_layers.0.branch1x1.1.weight         |    128     |\n",
            "|         stage3.Conv_layers.0.branch1x1.1.bias          |    128     |\n",
            "|        stage3.Conv_layers.0.branch3x3.0.weight         |   442368   |\n",
            "|         stage3.Conv_layers.0.branch3x3.0.bias          |    128     |\n",
            "|        stage3.Conv_layers.0.branch3x3.1.weight         |    128     |\n",
            "|         stage3.Conv_layers.0.branch3x3.1.bias          |    128     |\n",
            "|        stage3.Conv_layers.0.branch5x5.0.weight         |  1228800   |\n",
            "|         stage3.Conv_layers.0.branch5x5.0.bias          |    128     |\n",
            "|        stage3.Conv_layers.0.branch5x5.1.weight         |    128     |\n",
            "|         stage3.Conv_layers.0.branch5x5.1.bias          |    128     |\n",
            "|        stage3.Conv_layers.0.branch7x7.0.weight         |  2408448   |\n",
            "|         stage3.Conv_layers.0.branch7x7.0.bias          |    128     |\n",
            "|        stage3.Conv_layers.0.branch7x7.1.weight         |    128     |\n",
            "|         stage3.Conv_layers.0.branch7x7.1.bias          |    128     |\n",
            "|        stage3.Conv_layers.1.branch1x1.0.weight         |   49152    |\n",
            "|        stage3.Conv_layers.1.branch1x1.1.weight         |    128     |\n",
            "|         stage3.Conv_layers.1.branch1x1.1.bias          |    128     |\n",
            "|        stage3.Conv_layers.1.branch3x3.0.weight         |   442368   |\n",
            "|         stage3.Conv_layers.1.branch3x3.0.bias          |    128     |\n",
            "|        stage3.Conv_layers.1.branch3x3.1.weight         |    128     |\n",
            "|         stage3.Conv_layers.1.branch3x3.1.bias          |    128     |\n",
            "|        stage3.Conv_layers.1.branch5x5.0.weight         |  1228800   |\n",
            "|         stage3.Conv_layers.1.branch5x5.0.bias          |    128     |\n",
            "|        stage3.Conv_layers.1.branch5x5.1.weight         |    128     |\n",
            "|         stage3.Conv_layers.1.branch5x5.1.bias          |    128     |\n",
            "|        stage3.Conv_layers.1.branch7x7.0.weight         |  2408448   |\n",
            "|         stage3.Conv_layers.1.branch7x7.0.bias          |    128     |\n",
            "|        stage3.Conv_layers.1.branch7x7.1.weight         |    128     |\n",
            "|         stage3.Conv_layers.1.branch7x7.1.bias          |    128     |\n",
            "|   stage3.TB_layers.0.attention_block.fn.norm.weight    |    384     |\n",
            "|    stage3.TB_layers.0.attention_block.fn.norm.bias     |    384     |\n",
            "| stage3.TB_layers.0.attention_block.fn.fn.pos_embedding |    169     |\n",
            "| stage3.TB_layers.0.attention_block.fn.fn.to_qkv.weight |   442368   |\n",
            "| stage3.TB_layers.0.attention_block.fn.fn.to_out.weight |   147456   |\n",
            "|  stage3.TB_layers.0.attention_block.fn.fn.to_out.bias  |    384     |\n",
            "|      stage3.TB_layers.0.mlp_block.fn.norm.weight       |    384     |\n",
            "|       stage3.TB_layers.0.mlp_block.fn.norm.bias        |    384     |\n",
            "|    stage3.TB_layers.0.mlp_block.fn.fn.net.0.weight     |   589824   |\n",
            "|     stage3.TB_layers.0.mlp_block.fn.fn.net.0.bias      |    1536    |\n",
            "|    stage3.TB_layers.0.mlp_block.fn.fn.net.2.weight     |   589824   |\n",
            "|     stage3.TB_layers.0.mlp_block.fn.fn.net.2.bias      |    384     |\n",
            "|   stage3.TB_layers.1.attention_block.fn.norm.weight    |    384     |\n",
            "|    stage3.TB_layers.1.attention_block.fn.norm.bias     |    384     |\n",
            "| stage3.TB_layers.1.attention_block.fn.fn.pos_embedding |    169     |\n",
            "| stage3.TB_layers.1.attention_block.fn.fn.to_qkv.weight |   442368   |\n",
            "| stage3.TB_layers.1.attention_block.fn.fn.to_out.weight |   147456   |\n",
            "|  stage3.TB_layers.1.attention_block.fn.fn.to_out.bias  |    384     |\n",
            "|      stage3.TB_layers.1.mlp_block.fn.norm.weight       |    384     |\n",
            "|       stage3.TB_layers.1.mlp_block.fn.norm.bias        |    384     |\n",
            "|    stage3.TB_layers.1.mlp_block.fn.fn.net.0.weight     |   589824   |\n",
            "|     stage3.TB_layers.1.mlp_block.fn.fn.net.0.bias      |    1536    |\n",
            "|    stage3.TB_layers.1.mlp_block.fn.fn.net.2.weight     |   589824   |\n",
            "|     stage3.TB_layers.1.mlp_block.fn.fn.net.2.bias      |    384     |\n",
            "|              stage4.mg.patch_merge.weight              |  2654208   |\n",
            "|        stage4.Conv_layers.0.branch1x1.0.weight         |   196608   |\n",
            "|        stage4.Conv_layers.0.branch1x1.1.weight         |    256     |\n",
            "|         stage4.Conv_layers.0.branch1x1.1.bias          |    256     |\n",
            "|        stage4.Conv_layers.0.branch3x3.0.weight         |  1769472   |\n",
            "|         stage4.Conv_layers.0.branch3x3.0.bias          |    256     |\n",
            "|        stage4.Conv_layers.0.branch3x3.1.weight         |    256     |\n",
            "|         stage4.Conv_layers.0.branch3x3.1.bias          |    256     |\n",
            "|        stage4.Conv_layers.0.branch5x5.0.weight         |  4915200   |\n",
            "|         stage4.Conv_layers.0.branch5x5.0.bias          |    256     |\n",
            "|        stage4.Conv_layers.0.branch5x5.1.weight         |    256     |\n",
            "|         stage4.Conv_layers.0.branch5x5.1.bias          |    256     |\n",
            "|        stage4.Conv_layers.0.branch7x7.0.weight         |  9633792   |\n",
            "|         stage4.Conv_layers.0.branch7x7.0.bias          |    256     |\n",
            "|        stage4.Conv_layers.0.branch7x7.1.weight         |    256     |\n",
            "|         stage4.Conv_layers.0.branch7x7.1.bias          |    256     |\n",
            "|        stage4.Conv_layers.1.branch1x1.0.weight         |   196608   |\n",
            "|        stage4.Conv_layers.1.branch1x1.1.weight         |    256     |\n",
            "|         stage4.Conv_layers.1.branch1x1.1.bias          |    256     |\n",
            "|        stage4.Conv_layers.1.branch3x3.0.weight         |  1769472   |\n",
            "|         stage4.Conv_layers.1.branch3x3.0.bias          |    256     |\n",
            "|        stage4.Conv_layers.1.branch3x3.1.weight         |    256     |\n",
            "|         stage4.Conv_layers.1.branch3x3.1.bias          |    256     |\n",
            "|        stage4.Conv_layers.1.branch5x5.0.weight         |  4915200   |\n",
            "|         stage4.Conv_layers.1.branch5x5.0.bias          |    256     |\n",
            "|        stage4.Conv_layers.1.branch5x5.1.weight         |    256     |\n",
            "|         stage4.Conv_layers.1.branch5x5.1.bias          |    256     |\n",
            "|        stage4.Conv_layers.1.branch7x7.0.weight         |  9633792   |\n",
            "|         stage4.Conv_layers.1.branch7x7.0.bias          |    256     |\n",
            "|        stage4.Conv_layers.1.branch7x7.1.weight         |    256     |\n",
            "|         stage4.Conv_layers.1.branch7x7.1.bias          |    256     |\n",
            "|   stage4.TB_layers.0.attention_block.fn.norm.weight    |    768     |\n",
            "|    stage4.TB_layers.0.attention_block.fn.norm.bias     |    768     |\n",
            "| stage4.TB_layers.0.attention_block.fn.fn.pos_embedding |    169     |\n",
            "| stage4.TB_layers.0.attention_block.fn.fn.to_qkv.weight |  1769472   |\n",
            "| stage4.TB_layers.0.attention_block.fn.fn.to_out.weight |   589824   |\n",
            "|  stage4.TB_layers.0.attention_block.fn.fn.to_out.bias  |    768     |\n",
            "|      stage4.TB_layers.0.mlp_block.fn.norm.weight       |    768     |\n",
            "|       stage4.TB_layers.0.mlp_block.fn.norm.bias        |    768     |\n",
            "|    stage4.TB_layers.0.mlp_block.fn.fn.net.0.weight     |  2359296   |\n",
            "|     stage4.TB_layers.0.mlp_block.fn.fn.net.0.bias      |    3072    |\n",
            "|    stage4.TB_layers.0.mlp_block.fn.fn.net.2.weight     |  2359296   |\n",
            "|     stage4.TB_layers.0.mlp_block.fn.fn.net.2.bias      |    768     |\n",
            "|   stage4.TB_layers.1.attention_block.fn.norm.weight    |    768     |\n",
            "|    stage4.TB_layers.1.attention_block.fn.norm.bias     |    768     |\n",
            "| stage4.TB_layers.1.attention_block.fn.fn.pos_embedding |    169     |\n",
            "| stage4.TB_layers.1.attention_block.fn.fn.to_qkv.weight |  1769472   |\n",
            "| stage4.TB_layers.1.attention_block.fn.fn.to_out.weight |   589824   |\n",
            "|  stage4.TB_layers.1.attention_block.fn.fn.to_out.bias  |    768     |\n",
            "|      stage4.TB_layers.1.mlp_block.fn.norm.weight       |    768     |\n",
            "|       stage4.TB_layers.1.mlp_block.fn.norm.bias        |    768     |\n",
            "|    stage4.TB_layers.1.mlp_block.fn.fn.net.0.weight     |  2359296   |\n",
            "|     stage4.TB_layers.1.mlp_block.fn.fn.net.0.bias      |    3072    |\n",
            "|    stage4.TB_layers.1.mlp_block.fn.fn.net.2.weight     |  2359296   |\n",
            "|     stage4.TB_layers.1.mlp_block.fn.fn.net.2.bias      |    768     |\n",
            "|                   mlp_head.0.weight                    |    768     |\n",
            "|                    mlp_head.0.bias                     |    768     |\n",
            "|                   mlp_head.1.weight                    |   768000   |\n",
            "|                    mlp_head.1.bias                     |    1000    |\n",
            "+--------------------------------------------------------+------------+\n"
          ]
        }
      ],
      "source": [
        "# model = swin_t_fpn(hidden_dim=96, TB_layers=(2, 2, 6, 2), heads=(3, 6, 12, 24))\n",
        "model = HyneterForFPN(hidden_dim=128, Conv_layers=(2, 2, 6, 2),TB_layers=(2, 2, 18, 2), heads=(3, 6, 12, 24))\n",
        "model = HyneterForFPN(hidden_dim=128, Conv_layers=(2, 2, 6, 2),TB_layers=(2, 2, 18, 2), heads=(2, 4, 8, 16))\n",
        "model = HyneterForFPN(hidden_dim=96, Conv_layers=(2, 2, 3, 2),TB_layers=(2, 2, 6, 2), heads=(3, 6, 12, 24))\n",
        "model = HyneterForFPN(hidden_dim=96, Conv_layers=(2, 2, 2, 2),TB_layers=(2, 2, 2, 2), heads=(3, 6, 12, 24))\n",
        "model = hyneter_base()\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts the total number of parameters in a PyTorch model.\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def count_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts only the trainable parameters in a PyTorch model.\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "trainable_params = count_trainable_parameters(model)\n",
        "\n",
        "print(f\"Total parameters: {total_params / 1e6:.2f} M\")\n",
        "print(f\"Trainable parameters: {trainable_params / 1e6:.2f} M\")\n",
        "\n",
        "# For a more detailed breakdown (like `model.summary()` in Keras):\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def model_summary_pytorch(model):\n",
        "    table = PrettyTable([\"Module\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params += params\n",
        "    print(table)\n",
        "    # print(f\"\\nTotal Trainable Parameters: {total_params / 1e6:.2f} M\")\n",
        "\n",
        "model_summary_pytorch(model) # Uncomment to see detailed summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Hyneter(\n",
              "  (stage1): HyneterModule(\n",
              "    (mg): MultiGranularitySummingBlock(\n",
              "      (patch_merge): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "    (DualSwitching): DualSwitch_SwapOnly()\n",
              "    (TB_layers): ModuleList(\n",
              "      (0-1): 2 x TransformerBlock(\n",
              "        (attention_block): Residual(\n",
              "          (fn): PreNorm(\n",
              "            (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "            (fn): WindowAttention(\n",
              "              (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
              "              (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (mlp_block): Residual(\n",
              "          (fn): PreNorm(\n",
              "            (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=96, out_features=384, bias=True)\n",
              "                (1): GELU(approximate='none')\n",
              "                (2): Linear(in_features=384, out_features=96, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (stage2): HyneterModule(\n",
              "    (mg): MultiGranularitySummingBlock(\n",
              "      (patch_merge): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "    (DualSwitching): DualSwitch_SwapOnly()\n",
              "    (TB_layers): ModuleList(\n",
              "      (0-1): 2 x TransformerBlock(\n",
              "        (attention_block): Residual(\n",
              "          (fn): PreNorm(\n",
              "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "            (fn): WindowAttention(\n",
              "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "              (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (mlp_block): Residual(\n",
              "          (fn): PreNorm(\n",
              "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=192, out_features=768, bias=True)\n",
              "                (1): GELU(approximate='none')\n",
              "                (2): Linear(in_features=768, out_features=192, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (stage3): HyneterModule_DualSwitch(\n",
              "    (mg): MultiGranularitySummingBlock(\n",
              "      (patch_merge): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "    (Conv_layers): ModuleList(\n",
              "      (0-1): 2 x CNN(\n",
              "        (branch1x1): Sequential(\n",
              "          (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "        (branch3x3): Sequential(\n",
              "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (branch5x5): Sequential(\n",
              "          (0): Conv2d(384, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (branch7x7): Sequential(\n",
              "          (0): Conv2d(384, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (DualSwitching): DualSwitch_SwapOnly()\n",
              "    (TB_layers): ModuleList(\n",
              "      (0-1): 2 x TransformerBlock(\n",
              "        (attention_block): Residual(\n",
              "          (fn): PreNorm(\n",
              "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "            (fn): WindowAttention(\n",
              "              (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
              "              (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (mlp_block): Residual(\n",
              "          (fn): PreNorm(\n",
              "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "                (1): GELU(approximate='none')\n",
              "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (stage4): HyneterModule_DualSwitch(\n",
              "    (mg): MultiGranularitySummingBlock(\n",
              "      (patch_merge): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    )\n",
              "    (Conv_layers): ModuleList(\n",
              "      (0-1): 2 x CNN(\n",
              "        (branch1x1): Sequential(\n",
              "          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "        (branch3x3): Sequential(\n",
              "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (branch5x5): Sequential(\n",
              "          (0): Conv2d(768, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (branch7x7): Sequential(\n",
              "          (0): Conv2d(768, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (DualSwitching): DualSwitch_SwapOnly()\n",
              "    (TB_layers): ModuleList(\n",
              "      (0-1): 2 x TransformerBlock(\n",
              "        (attention_block): Residual(\n",
              "          (fn): PreNorm(\n",
              "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fn): WindowAttention(\n",
              "              (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (mlp_block): Residual(\n",
              "          (fn): PreNorm(\n",
              "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fn): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (1): GELU(approximate='none')\n",
              "                (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mlp_head): Sequential(\n",
              "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=768, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test on x = torch.randn(1, 3, 224, 224).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "HyneterModule: HNB: Patch -> TB\n",
            "HyneterModule no CNN forward pass\n",
            "\n",
            "Input shape before Multigranularity CNN: torch.Size([1, 3, 224, 224])\n",
            "Input shape after Multigranularity CNN: torch.Size([1, 96, 224, 224])\n",
            "X(TB) shape before Transformer Blocks: torch.Size([1, 224, 224, 96])\n",
            "Input shape after Transformer Blocks: torch.Size([1, 224, 224, 96])\n",
            "X(TB) shape: torch.Size([1, 96, 224, 224])\n",
            "\n",
            "HyneterModule: HNB: Patch -> TB\n",
            "HyneterModule no CNN forward pass\n",
            "\n",
            "Input shape before Multigranularity CNN: torch.Size([1, 96, 224, 224])\n",
            "Input shape after Multigranularity CNN: torch.Size([1, 192, 224, 224])\n",
            "X(TB) shape before Transformer Blocks: torch.Size([1, 224, 224, 192])\n",
            "Input shape after Transformer Blocks: torch.Size([1, 224, 224, 192])\n",
            "X(TB) shape: torch.Size([1, 192, 224, 224])\n",
            "\n",
            "HyneterModule: HNB: Patch -> Conv -> TB\n",
            "HyneterModule forward pass\n",
            "\n",
            "Input shape before Multigranularity CNN: torch.Size([1, 192, 224, 224])\n",
            "Input shape after Multigranularity CNN: torch.Size([1, 384, 224, 224])\n",
            "X(CNN) shape before Conv layers: torch.Size([1, 384, 224, 224])\n",
            "X(CNN) shape after Conv layers: torch.Size([1, 384, 224, 224])\n",
            "X(TB) shape before Transformer Blocks: torch.Size([1, 224, 224, 384])\n",
            "Input shape after Transformer Blocks: torch.Size([1, 224, 224, 384])\n",
            "########### going to calculate Z ###########\n",
            "X(CNN) shape: torch.Size([1, 384, 224, 224])\n",
            "X(TB) shape: torch.Size([1, 384, 224, 224])\n",
            "Z shape torch.Size([1, 384, 224, 224])\n",
            "Z before tanh: tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           -0.0000e+00,  0.0000e+00],\n",
            "          [ 8.1981e-03,  1.4658e-02,  2.2333e-02,  ...,  4.4838e-02,\n",
            "            0.0000e+00,  2.7474e-02],\n",
            "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "            6.3505e-03,  1.5132e-02],\n",
            "          ...,\n",
            "          [ 1.8437e-02,  6.0142e-02,  2.1366e-02,  ...,  8.9908e-03,\n",
            "            4.7447e-03,  4.4324e-02],\n",
            "          [ 0.0000e+00,  6.5993e-02,  5.3600e-02,  ...,  2.5859e-02,\n",
            "            7.5444e-03,  0.0000e+00],\n",
            "          [ 1.7366e-02,  6.0119e-03,  1.7790e-02,  ...,  5.3081e-02,\n",
            "            2.4187e-02,  2.5221e-02]],\n",
            "\n",
            "         [[-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -7.9662e-03,\n",
            "           -9.7977e-03, -1.4174e-03],\n",
            "          [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  4.6803e-03,\n",
            "            1.8318e-04, -2.8407e-02],\n",
            "          [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
            "           -2.2227e-03, -0.0000e+00],\n",
            "          ...,\n",
            "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.3451e-03,\n",
            "           -5.0686e-04, -2.1716e-02],\n",
            "          [-0.0000e+00, -4.8034e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           -0.0000e+00, -1.2691e-02],\n",
            "          [ 0.0000e+00, -0.0000e+00, -2.5423e-03,  ..., -0.0000e+00,\n",
            "           -3.0491e-03,  0.0000e+00]],\n",
            "\n",
            "         [[-3.6526e-05, -0.0000e+00, -0.0000e+00,  ...,  5.6199e-03,\n",
            "           -9.1117e-05,  0.0000e+00],\n",
            "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  1.9650e-02,\n",
            "            2.1272e-03,  0.0000e+00],\n",
            "          [ 2.8799e-03, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
            "           -0.0000e+00,  0.0000e+00],\n",
            "          ...,\n",
            "          [-5.7128e-03,  0.0000e+00,  0.0000e+00,  ..., -6.1204e-03,\n",
            "            3.6325e-03, -1.4592e-03],\n",
            "          [ 9.6991e-04,  0.0000e+00, -6.3586e-03,  ...,  2.0642e-03,\n",
            "            2.3862e-03,  0.0000e+00],\n",
            "          [ 2.0625e-02,  0.0000e+00,  7.8035e-03,  ...,  1.3159e-02,\n",
            "           -1.3323e-03, -5.0786e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0000e+00,  0.0000e+00, -4.6701e-06,  ..., -5.8776e-03,\n",
            "           -0.0000e+00,  0.0000e+00],\n",
            "          [ 3.5340e-03, -7.7415e-04,  0.0000e+00,  ..., -0.0000e+00,\n",
            "           -8.4562e-03,  6.7123e-03],\n",
            "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.5392e-03,\n",
            "            0.0000e+00, -1.5723e-02],\n",
            "          ...,\n",
            "          [ 5.2110e-03,  9.0828e-03,  7.9887e-02,  ...,  1.2851e-03,\n",
            "            9.8798e-03,  5.9821e-03],\n",
            "          [ 5.8601e-03, -1.7968e-03,  2.8672e-02,  ..., -6.0563e-04,\n",
            "           -1.8227e-02,  5.7005e-04],\n",
            "          [ 1.4862e-02, -6.4745e-03,  1.7291e-02,  ...,  4.3736e-02,\n",
            "            2.9500e-02,  0.0000e+00]],\n",
            "\n",
            "         [[-0.0000e+00, -2.0265e-02, -1.5559e-02,  ...,  0.0000e+00,\n",
            "           -0.0000e+00,  1.7687e-03],\n",
            "          [ 0.0000e+00, -0.0000e+00,  3.7119e-03,  ...,  1.7342e-04,\n",
            "            0.0000e+00,  8.8231e-03],\n",
            "          [-0.0000e+00, -0.0000e+00,  2.5292e-02,  ..., -0.0000e+00,\n",
            "            6.5450e-03,  3.5012e-02],\n",
            "          ...,\n",
            "          [ 4.4716e-02,  2.8557e-03, -0.0000e+00,  ..., -0.0000e+00,\n",
            "           -2.2383e-03,  2.3927e-02],\n",
            "          [ 0.0000e+00, -2.9797e-03,  2.6587e-02,  ..., -1.1540e-02,\n",
            "            1.2159e-02,  2.2142e-02],\n",
            "          [-1.8520e-02,  4.6927e-03, -9.4973e-03,  ..., -2.4293e-02,\n",
            "           -2.4405e-02, -3.7231e-03]],\n",
            "\n",
            "         [[-1.7696e-02, -5.3299e-03, -1.7047e-02,  ..., -1.2370e-02,\n",
            "           -0.0000e+00, -0.0000e+00],\n",
            "          [-1.5036e-02, -2.8011e-03, -1.2838e-02,  ..., -5.2965e-02,\n",
            "           -7.5067e-04,  4.1954e-02],\n",
            "          [ 3.2885e-02, -2.7469e-02, -6.5330e-02,  ..., -3.2532e-02,\n",
            "           -4.0025e-03, -5.4424e-04],\n",
            "          ...,\n",
            "          [-3.0551e-02, -5.6464e-03,  9.3144e-03,  ...,  4.8817e-02,\n",
            "            8.3831e-03, -7.0747e-03],\n",
            "          [-3.4679e-02, -1.3938e-02, -0.0000e+00,  ..., -4.0872e-02,\n",
            "            1.4612e-02, -4.7634e-03],\n",
            "          [-3.7369e-02, -9.9669e-04, -3.7027e-02,  ..., -2.4522e-02,\n",
            "           -1.8218e-02, -1.4001e-02]]]], device='cuda:0')\n",
            "Z after tanh: tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           -0.0000e+00,  0.0000e+00],\n",
            "          [ 8.1979e-03,  1.4657e-02,  2.2329e-02,  ...,  4.4808e-02,\n",
            "            0.0000e+00,  2.7467e-02],\n",
            "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "            6.3504e-03,  1.5131e-02],\n",
            "          ...,\n",
            "          [ 1.8435e-02,  6.0070e-02,  2.1363e-02,  ...,  8.9906e-03,\n",
            "            4.7447e-03,  4.4295e-02],\n",
            "          [ 0.0000e+00,  6.5897e-02,  5.3549e-02,  ...,  2.5854e-02,\n",
            "            7.5443e-03,  0.0000e+00],\n",
            "          [ 1.7364e-02,  6.0119e-03,  1.7788e-02,  ...,  5.3031e-02,\n",
            "            2.4183e-02,  2.5216e-02]],\n",
            "\n",
            "         [[-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -7.9661e-03,\n",
            "           -9.7974e-03, -1.4174e-03],\n",
            "          [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  4.6802e-03,\n",
            "            1.8318e-04, -2.8399e-02],\n",
            "          [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
            "           -2.2227e-03, -0.0000e+00],\n",
            "          ...,\n",
            "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -3.3451e-03,\n",
            "           -5.0686e-04, -2.1712e-02],\n",
            "          [-0.0000e+00, -4.8033e-03,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           -0.0000e+00, -1.2691e-02],\n",
            "          [ 0.0000e+00, -0.0000e+00, -2.5423e-03,  ..., -0.0000e+00,\n",
            "           -3.0491e-03,  0.0000e+00]],\n",
            "\n",
            "         [[-3.6526e-05, -0.0000e+00, -0.0000e+00,  ...,  5.6198e-03,\n",
            "           -9.1117e-05,  0.0000e+00],\n",
            "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  1.9648e-02,\n",
            "            2.1272e-03,  0.0000e+00],\n",
            "          [ 2.8799e-03, -0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
            "           -0.0000e+00,  0.0000e+00],\n",
            "          ...,\n",
            "          [-5.7127e-03,  0.0000e+00,  0.0000e+00,  ..., -6.1204e-03,\n",
            "            3.6325e-03, -1.4592e-03],\n",
            "          [ 9.6991e-04,  0.0000e+00, -6.3585e-03,  ...,  2.0642e-03,\n",
            "            2.3862e-03,  0.0000e+00],\n",
            "          [ 2.0622e-02,  0.0000e+00,  7.8034e-03,  ...,  1.3158e-02,\n",
            "           -1.3323e-03, -5.0785e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0000e+00,  0.0000e+00, -4.6701e-06,  ..., -5.8776e-03,\n",
            "           -0.0000e+00,  0.0000e+00],\n",
            "          [ 3.5339e-03, -7.7415e-04,  0.0000e+00,  ..., -0.0000e+00,\n",
            "           -8.4560e-03,  6.7122e-03],\n",
            "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.5391e-03,\n",
            "            0.0000e+00, -1.5722e-02],\n",
            "          ...,\n",
            "          [ 5.2109e-03,  9.0825e-03,  7.9717e-02,  ...,  1.2851e-03,\n",
            "            9.8794e-03,  5.9820e-03],\n",
            "          [ 5.8601e-03, -1.7968e-03,  2.8665e-02,  ..., -6.0563e-04,\n",
            "           -1.8224e-02,  5.7005e-04],\n",
            "          [ 1.4861e-02, -6.4744e-03,  1.7290e-02,  ...,  4.3708e-02,\n",
            "            2.9492e-02,  0.0000e+00]],\n",
            "\n",
            "         [[-0.0000e+00, -2.0262e-02, -1.5557e-02,  ...,  0.0000e+00,\n",
            "           -0.0000e+00,  1.7687e-03],\n",
            "          [ 0.0000e+00, -0.0000e+00,  3.7119e-03,  ...,  1.7342e-04,\n",
            "            0.0000e+00,  8.8229e-03],\n",
            "          [-0.0000e+00, -0.0000e+00,  2.5286e-02,  ..., -0.0000e+00,\n",
            "            6.5450e-03,  3.4997e-02],\n",
            "          ...,\n",
            "          [ 4.4686e-02,  2.8557e-03, -0.0000e+00,  ..., -0.0000e+00,\n",
            "           -2.2383e-03,  2.3922e-02],\n",
            "          [ 0.0000e+00, -2.9797e-03,  2.6581e-02,  ..., -1.1540e-02,\n",
            "            1.2158e-02,  2.2138e-02],\n",
            "          [-1.8518e-02,  4.6927e-03, -9.4971e-03,  ..., -2.4289e-02,\n",
            "           -2.4400e-02, -3.7231e-03]],\n",
            "\n",
            "         [[-1.7694e-02, -5.3299e-03, -1.7046e-02,  ..., -1.2369e-02,\n",
            "           -0.0000e+00, -0.0000e+00],\n",
            "          [-1.5035e-02, -2.8011e-03, -1.2837e-02,  ..., -5.2916e-02,\n",
            "           -7.5067e-04,  4.1930e-02],\n",
            "          [ 3.2873e-02, -2.7462e-02, -6.5237e-02,  ..., -3.2520e-02,\n",
            "           -4.0024e-03, -5.4424e-04],\n",
            "          ...,\n",
            "          [-3.0542e-02, -5.6463e-03,  9.3142e-03,  ...,  4.8778e-02,\n",
            "            8.3829e-03, -7.0745e-03],\n",
            "          [-3.4665e-02, -1.3937e-02, -0.0000e+00,  ..., -4.0849e-02,\n",
            "            1.4611e-02, -4.7634e-03],\n",
            "          [-3.7352e-02, -9.9669e-04, -3.7010e-02,  ..., -2.4517e-02,\n",
            "           -1.8216e-02, -1.4000e-02]]]], device='cuda:0')\n",
            "Z is tanh(Dot product between x and S): torch.Size([1, 384, 224, 224])\n",
            "output shape after adding Z: torch.Size([1, 384, 224, 224])\n",
            "output value: tensor([[[[ 0.9107,  1.0183,  0.0037,  ...,  0.1894, -0.1030,  0.2459],\n",
            "          [ 0.7731,  0.3557,  0.4748,  ...,  0.5257,  0.2813,  0.6506],\n",
            "          [-0.0082,  0.7289,  0.2787,  ...,  0.4654,  0.1635,  0.4626],\n",
            "          ...,\n",
            "          [ 0.7349,  0.9405,  0.4904,  ...,  0.6970,  0.0591,  0.5666],\n",
            "          [ 0.7367,  0.8297,  1.0013,  ...,  0.9435,  0.4246,  0.7520],\n",
            "          [ 0.7550,  0.1194,  0.3647,  ...,  0.6290,  0.6219,  0.6555]],\n",
            "\n",
            "         [[-0.1965, -0.5476,  0.2281,  ..., -0.3699, -0.5967, -0.5751],\n",
            "          [-0.0291, -0.0052,  0.3033,  ...,  0.1592,  0.0721, -0.5056],\n",
            "          [-0.1180, -0.9114,  0.2150,  ..., -0.7049, -0.1698, -0.9101],\n",
            "          ...,\n",
            "          [-0.0370, -0.0959, -0.2249,  ..., -0.3519, -0.0971, -0.4245],\n",
            "          [-0.3187, -0.5669,  0.2270,  ...,  0.3285, -0.1405, -0.3654],\n",
            "          [ 0.5440, -0.4859, -0.5236,  ..., -0.3701, -0.0909,  0.1004]],\n",
            "\n",
            "         [[-0.0213, -0.5187, -0.7496,  ...,  0.1569, -0.0025,  0.1740],\n",
            "          [-0.5701, -0.0895, -0.5383,  ...,  0.6789,  0.4358,  0.8421],\n",
            "          [ 0.2123, -0.1121, -0.5330,  ...,  0.2193, -0.1037,  0.2599],\n",
            "          ...,\n",
            "          [-0.1131,  0.1122,  0.4195,  ..., -0.2344,  0.4997, -0.1309],\n",
            "          [ 0.2478,  0.4562, -0.0979,  ...,  0.1721,  0.0917,  0.2309],\n",
            "          [ 0.5125,  0.2878,  0.3573,  ...,  0.4688, -0.3944, -0.3972]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.4936,  0.3288, -0.0015,  ..., -0.3620, -0.3634,  0.1262],\n",
            "          [ 0.3372, -0.0219,  0.4013,  ..., -0.5674, -0.5208,  0.2606],\n",
            "          [ 0.1367,  0.0707,  0.4304,  ...,  0.0953,  0.0224, -0.5112],\n",
            "          ...,\n",
            "          [ 0.0734,  0.2263,  0.7075,  ...,  0.3656,  0.3414,  0.0915],\n",
            "          [ 0.0789, -0.0745,  0.3212,  ..., -0.0264, -0.4085,  0.0074],\n",
            "          [ 0.5424, -0.1422,  0.4343,  ...,  0.9818,  0.5663,  0.4738]],\n",
            "\n",
            "         [[-0.0513, -0.4368, -0.9167,  ...,  0.4441, -0.0556,  0.1820],\n",
            "          [ 0.7985, -0.0658,  0.2810,  ...,  0.0066,  0.7735,  0.2535],\n",
            "          [-0.3905, -0.3550,  0.3498,  ..., -0.1648,  0.3534,  0.6238],\n",
            "          ...,\n",
            "          [ 0.8826,  0.0715, -0.1004,  ..., -0.4917, -0.5460,  0.3050],\n",
            "          [ 0.1267, -0.1013,  0.5987,  ..., -0.1028,  0.1729,  0.2524],\n",
            "          [-0.3982,  0.0899, -0.6035,  ..., -0.3660, -0.3355, -0.2416]],\n",
            "\n",
            "         [[-0.3243, -0.5215, -0.6126,  ..., -0.5438, -0.6612, -0.0433],\n",
            "          [-0.3336, -0.2530, -0.3201,  ..., -0.6918, -0.0657,  0.6483],\n",
            "          [ 0.4274, -0.4805, -0.8005,  ..., -0.2803, -0.1068, -0.0260],\n",
            "          ...,\n",
            "          [-0.9668, -0.3756,  0.1729,  ...,  0.5788,  0.0872, -0.1694],\n",
            "          [-0.4427, -0.5077, -0.4991,  ..., -0.4070,  0.3286, -0.0961],\n",
            "          [-0.6203, -0.3774, -0.9008,  ..., -0.4539, -0.3077, -0.4333]]]],\n",
            "       device='cuda:0')\n",
            "\n",
            "HyneterModule: HNB: Patch -> Conv -> TB\n",
            "HyneterModule forward pass\n",
            "\n",
            "Input shape before Multigranularity CNN: torch.Size([1, 384, 224, 224])\n",
            "Input shape after Multigranularity CNN: torch.Size([1, 768, 224, 224])\n",
            "X(CNN) shape before Conv layers: torch.Size([1, 768, 224, 224])\n",
            "X(CNN) shape after Conv layers: torch.Size([1, 768, 224, 224])\n",
            "X(TB) shape before Transformer Blocks: torch.Size([1, 224, 224, 768])\n",
            "Input shape after Transformer Blocks: torch.Size([1, 224, 224, 768])\n",
            "########### going to calculate Z ###########\n",
            "X(CNN) shape: torch.Size([1, 768, 224, 224])\n",
            "X(TB) shape: torch.Size([1, 768, 224, 224])\n",
            "Z shape torch.Size([1, 768, 224, 224])\n",
            "Z before tanh: tensor([[[[ 1.5057e-02, -4.0231e-04,  7.9733e-03,  ...,  4.4306e-02,\n",
            "            1.4089e-02,  1.3163e-02],\n",
            "          [ 4.1076e-03,  4.2700e-02,  0.0000e+00,  ...,  3.3494e-02,\n",
            "            3.6421e-02,  0.0000e+00],\n",
            "          [ 0.0000e+00,  1.5191e-02,  1.4222e-02,  ...,  1.1098e-02,\n",
            "            0.0000e+00,  0.0000e+00],\n",
            "          ...,\n",
            "          [ 0.0000e+00,  2.3511e-02,  3.0360e-03,  ...,  0.0000e+00,\n",
            "            5.4775e-02,  1.3687e-02],\n",
            "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "            3.7920e-02,  0.0000e+00],\n",
            "          [ 0.0000e+00,  1.0912e-02,  0.0000e+00,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  1.3109e-02]],\n",
            "\n",
            "         [[ 3.5970e-02,  4.3738e-02,  0.0000e+00,  ...,  7.6475e-03,\n",
            "            0.0000e+00,  1.4296e-03],\n",
            "          [ 1.1827e-01,  1.0682e-01,  5.6884e-02,  ...,  1.6510e-02,\n",
            "            5.6892e-02,  3.9984e-02],\n",
            "          [ 6.3426e-02,  4.6203e-02,  3.7707e-02,  ...,  5.8947e-02,\n",
            "            4.9525e-02,  3.1530e-02],\n",
            "          ...,\n",
            "          [-3.2912e-03,  1.0199e-02,  2.6519e-04,  ...,  7.8842e-02,\n",
            "            5.3729e-03,  2.8358e-02],\n",
            "          [-4.2446e-03,  2.7127e-03,  1.6303e-02,  ...,  2.5965e-02,\n",
            "            5.0646e-02,  2.5240e-02],\n",
            "          [-1.5807e-04,  1.3726e-02,  8.7412e-03,  ...,  3.6571e-02,\n",
            "            2.7142e-02,  4.3924e-03]],\n",
            "\n",
            "         [[ 2.5733e-02,  9.1823e-03,  1.9853e-02,  ...,  6.4060e-03,\n",
            "            1.9056e-02, -9.4346e-03],\n",
            "          [ 3.8790e-03,  1.5600e-02,  1.1771e-02,  ..., -8.5733e-03,\n",
            "            3.5254e-03,  1.8087e-02],\n",
            "          [ 4.2150e-03,  5.0006e-03,  1.7695e-02,  ...,  7.0277e-03,\n",
            "           -1.9128e-02,  5.1538e-04],\n",
            "          ...,\n",
            "          [ 8.6581e-03,  1.6746e-02,  3.0549e-02,  ...,  3.6751e-03,\n",
            "            2.8276e-02,  1.2611e-02],\n",
            "          [ 2.1585e-03,  4.2004e-02,  0.0000e+00,  ...,  1.0384e-02,\n",
            "            1.1050e-02,  3.5063e-03],\n",
            "          [ 0.0000e+00,  0.0000e+00, -1.4139e-03,  ...,  3.6696e-05,\n",
            "            1.0738e-02,  6.2604e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0000e+00, -3.4952e-03, -4.2046e-03,  ..., -0.0000e+00,\n",
            "           -0.0000e+00, -0.0000e+00],\n",
            "          [-1.7607e-02, -4.6636e-02, -4.4272e-02,  ..., -0.0000e+00,\n",
            "           -0.0000e+00, -3.8208e-03],\n",
            "          [-1.4430e-02, -4.7006e-02, -2.0501e-02,  ..., -5.5544e-02,\n",
            "           -0.0000e+00, -2.9968e-02],\n",
            "          ...,\n",
            "          [-6.3047e-02, -7.2306e-02, -2.3746e-02,  ..., -9.9192e-03,\n",
            "           -1.8429e-02, -4.8923e-02],\n",
            "          [-1.6468e-02, -6.5591e-02, -6.0363e-02,  ..., -1.1533e-02,\n",
            "           -5.3745e-02, -1.3615e-02],\n",
            "          [-1.9832e-02, -6.5454e-02, -2.8472e-02,  ..., -4.0418e-02,\n",
            "           -5.6606e-03, -2.4342e-02]],\n",
            "\n",
            "         [[-0.0000e+00,  3.3920e-03, -0.0000e+00,  ..., -0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00],\n",
            "          [-0.0000e+00,  8.3583e-03, -0.0000e+00,  ...,  0.0000e+00,\n",
            "           -7.9318e-05, -7.1396e-04],\n",
            "          [-0.0000e+00,  0.0000e+00, -1.0276e-03,  ..., -8.9879e-04,\n",
            "           -8.4368e-03, -0.0000e+00],\n",
            "          ...,\n",
            "          [ 0.0000e+00,  0.0000e+00, -1.5149e-03,  ..., -9.5025e-03,\n",
            "            5.5002e-04,  1.4958e-04],\n",
            "          [-1.0517e-03,  0.0000e+00, -0.0000e+00,  ..., -1.2517e-02,\n",
            "           -8.8393e-04,  3.9094e-02],\n",
            "          [-0.0000e+00,  0.0000e+00,  3.1411e-05,  ...,  7.1302e-03,\n",
            "            6.1237e-03, -6.3187e-03]],\n",
            "\n",
            "         [[ 9.9992e-03,  8.9638e-03, -7.0798e-04,  ...,  0.0000e+00,\n",
            "           -0.0000e+00, -0.0000e+00],\n",
            "          [ 3.0536e-03, -2.7275e-03, -1.4057e-02,  ...,  0.0000e+00,\n",
            "           -0.0000e+00,  0.0000e+00],\n",
            "          [-0.0000e+00,  9.5468e-03,  2.9456e-03,  ..., -1.1926e-03,\n",
            "           -0.0000e+00, -0.0000e+00],\n",
            "          ...,\n",
            "          [-2.6272e-03, -7.5736e-04, -7.4043e-03,  ...,  5.2608e-03,\n",
            "           -0.0000e+00, -1.8237e-04],\n",
            "          [ 6.2018e-03,  2.3927e-03,  1.9940e-02,  ..., -4.1229e-03,\n",
            "            0.0000e+00, -4.4217e-03],\n",
            "          [ 9.7585e-03,  1.0506e-02,  7.1198e-03,  ...,  1.3600e-03,\n",
            "           -0.0000e+00, -6.7215e-03]]]], device='cuda:0')\n",
            "Z after tanh: tensor([[[[ 1.5056e-02, -4.0231e-04,  7.9731e-03,  ...,  4.4277e-02,\n",
            "            1.4088e-02,  1.3163e-02],\n",
            "          [ 4.1076e-03,  4.2674e-02,  0.0000e+00,  ...,  3.3482e-02,\n",
            "            3.6405e-02,  0.0000e+00],\n",
            "          [ 0.0000e+00,  1.5190e-02,  1.4222e-02,  ...,  1.1098e-02,\n",
            "            0.0000e+00,  0.0000e+00],\n",
            "          ...,\n",
            "          [ 0.0000e+00,  2.3507e-02,  3.0360e-03,  ...,  0.0000e+00,\n",
            "            5.4720e-02,  1.3686e-02],\n",
            "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "            3.7902e-02,  0.0000e+00],\n",
            "          [ 0.0000e+00,  1.0912e-02,  0.0000e+00,  ...,  0.0000e+00,\n",
            "            0.0000e+00,  1.3108e-02]],\n",
            "\n",
            "         [[ 3.5955e-02,  4.3711e-02,  0.0000e+00,  ...,  7.6473e-03,\n",
            "            0.0000e+00,  1.4296e-03],\n",
            "          [ 1.1772e-01,  1.0642e-01,  5.6823e-02,  ...,  1.6508e-02,\n",
            "            5.6830e-02,  3.9962e-02],\n",
            "          [ 6.3342e-02,  4.6170e-02,  3.7689e-02,  ...,  5.8879e-02,\n",
            "            4.9484e-02,  3.1520e-02],\n",
            "          ...,\n",
            "          [-3.2912e-03,  1.0199e-02,  2.6519e-04,  ...,  7.8679e-02,\n",
            "            5.3729e-03,  2.8350e-02],\n",
            "          [-4.2445e-03,  2.7127e-03,  1.6302e-02,  ...,  2.5959e-02,\n",
            "            5.0602e-02,  2.5235e-02],\n",
            "          [-1.5807e-04,  1.3726e-02,  8.7410e-03,  ...,  3.6555e-02,\n",
            "            2.7136e-02,  4.3924e-03]],\n",
            "\n",
            "         [[ 2.5728e-02,  9.1820e-03,  1.9850e-02,  ...,  6.4059e-03,\n",
            "            1.9054e-02, -9.4343e-03],\n",
            "          [ 3.8790e-03,  1.5598e-02,  1.1770e-02,  ..., -8.5731e-03,\n",
            "            3.5254e-03,  1.8085e-02],\n",
            "          [ 4.2149e-03,  5.0006e-03,  1.7693e-02,  ...,  7.0276e-03,\n",
            "           -1.9126e-02,  5.1538e-04],\n",
            "          ...,\n",
            "          [ 8.6578e-03,  1.6745e-02,  3.0540e-02,  ...,  3.6751e-03,\n",
            "            2.8268e-02,  1.2611e-02],\n",
            "          [ 2.1585e-03,  4.1979e-02,  0.0000e+00,  ...,  1.0384e-02,\n",
            "            1.1049e-02,  3.5063e-03],\n",
            "          [ 0.0000e+00,  0.0000e+00, -1.4139e-03,  ...,  3.6696e-05,\n",
            "            1.0737e-02,  6.2603e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0000e+00, -3.4952e-03, -4.2045e-03,  ..., -0.0000e+00,\n",
            "           -0.0000e+00, -0.0000e+00],\n",
            "          [-1.7605e-02, -4.6602e-02, -4.4243e-02,  ..., -0.0000e+00,\n",
            "           -0.0000e+00, -3.8208e-03],\n",
            "          [-1.4429e-02, -4.6972e-02, -2.0498e-02,  ..., -5.5487e-02,\n",
            "           -0.0000e+00, -2.9959e-02],\n",
            "          ...,\n",
            "          [-6.2964e-02, -7.2180e-02, -2.3741e-02,  ..., -9.9188e-03,\n",
            "           -1.8427e-02, -4.8884e-02],\n",
            "          [-1.6467e-02, -6.5497e-02, -6.0290e-02,  ..., -1.1532e-02,\n",
            "           -5.3693e-02, -1.3614e-02],\n",
            "          [-1.9829e-02, -6.5360e-02, -2.8465e-02,  ..., -4.0396e-02,\n",
            "           -5.6605e-03, -2.4337e-02]],\n",
            "\n",
            "         [[-0.0000e+00,  3.3920e-03, -0.0000e+00,  ..., -0.0000e+00,\n",
            "            0.0000e+00,  0.0000e+00],\n",
            "          [-0.0000e+00,  8.3581e-03, -0.0000e+00,  ...,  0.0000e+00,\n",
            "           -7.9318e-05, -7.1396e-04],\n",
            "          [-0.0000e+00,  0.0000e+00, -1.0276e-03,  ..., -8.9879e-04,\n",
            "           -8.4366e-03, -0.0000e+00],\n",
            "          ...,\n",
            "          [ 0.0000e+00,  0.0000e+00, -1.5149e-03,  ..., -9.5023e-03,\n",
            "            5.5002e-04,  1.4958e-04],\n",
            "          [-1.0517e-03,  0.0000e+00, -0.0000e+00,  ..., -1.2517e-02,\n",
            "           -8.8393e-04,  3.9074e-02],\n",
            "          [-0.0000e+00,  0.0000e+00,  3.1411e-05,  ...,  7.1301e-03,\n",
            "            6.1236e-03, -6.3186e-03]],\n",
            "\n",
            "         [[ 9.9988e-03,  8.9636e-03, -7.0798e-04,  ...,  0.0000e+00,\n",
            "           -0.0000e+00, -0.0000e+00],\n",
            "          [ 3.0536e-03, -2.7275e-03, -1.4056e-02,  ...,  0.0000e+00,\n",
            "           -0.0000e+00,  0.0000e+00],\n",
            "          [-0.0000e+00,  9.5465e-03,  2.9456e-03,  ..., -1.1926e-03,\n",
            "           -0.0000e+00, -0.0000e+00],\n",
            "          ...,\n",
            "          [-2.6272e-03, -7.5736e-04, -7.4042e-03,  ...,  5.2608e-03,\n",
            "           -0.0000e+00, -1.8237e-04],\n",
            "          [ 6.2017e-03,  2.3927e-03,  1.9938e-02,  ..., -4.1229e-03,\n",
            "            0.0000e+00, -4.4217e-03],\n",
            "          [ 9.7582e-03,  1.0505e-02,  7.1197e-03,  ...,  1.3600e-03,\n",
            "           -0.0000e+00, -6.7214e-03]]]], device='cuda:0')\n",
            "Z is tanh(Dot product between x and S): torch.Size([1, 768, 224, 224])\n",
            "output shape after adding Z: torch.Size([1, 768, 224, 224])\n",
            "output value: tensor([[[[ 7.9296e-01, -6.9233e-03,  4.6542e-01,  ...,  9.4707e-01,\n",
            "            5.1542e-01,  6.0277e-01],\n",
            "          [ 2.5318e-01,  6.7828e-01,  5.0876e-01,  ...,  7.4405e-01,\n",
            "            8.5073e-01,  3.3203e-01],\n",
            "          [ 3.0539e-01,  2.3936e-01,  1.1129e+00,  ...,  3.4000e-01,\n",
            "            8.6159e-01,  4.6028e-01],\n",
            "          ...,\n",
            "          [ 1.2901e-01,  7.3341e-01,  4.9488e-01,  ...,  5.4506e-01,\n",
            "            1.4123e+00,  7.2248e-01],\n",
            "          [ 1.1008e+00,  1.1629e+00,  9.6864e-01,  ...,  5.9128e-01,\n",
            "            8.4708e-01,  7.7655e-01],\n",
            "          [ 5.2761e-01,  4.8166e-01,  8.5617e-01,  ...,  8.9760e-01,\n",
            "            7.8193e-01,  8.9526e-01]],\n",
            "\n",
            "         [[ 6.1627e-01,  7.7659e-01,  7.1874e-01,  ...,  7.0789e-01,\n",
            "            7.7705e-01,  2.5027e-01],\n",
            "          [ 1.1794e+00,  9.7680e-01,  9.1720e-01,  ...,  3.8850e-01,\n",
            "            9.3699e-01,  9.6658e-01],\n",
            "          [ 6.7272e-01,  6.0453e-01,  4.4099e-01,  ...,  6.7522e-01,\n",
            "            5.6931e-01,  6.4214e-01],\n",
            "          ...,\n",
            "          [-4.9480e-02,  1.4499e-01,  5.4654e-03,  ...,  8.7993e-01,\n",
            "            3.0710e-01,  5.5970e-01],\n",
            "          [-4.9754e-02,  3.0515e-01,  3.7554e-01,  ...,  4.0913e-01,\n",
            "            6.1250e-01,  3.8893e-01],\n",
            "          [-3.3284e-03,  3.8556e-01,  2.3102e-01,  ...,  5.4741e-01,\n",
            "            4.7664e-01,  2.2958e-01]],\n",
            "\n",
            "         [[ 6.8600e-01,  1.3267e-01,  2.3436e-01,  ...,  1.5436e-01,\n",
            "            3.3281e-01, -1.9836e-01],\n",
            "          [ 1.9513e-01,  4.1170e-01,  4.3830e-01,  ..., -6.9774e-02,\n",
            "            1.1305e-01,  2.2351e-01],\n",
            "          [ 1.5582e-01,  2.7395e-01,  4.7967e-01,  ...,  3.1141e-01,\n",
            "           -3.0771e-01,  5.3267e-03],\n",
            "          ...,\n",
            "          [ 3.7512e-01,  3.0337e-01,  3.9910e-01,  ...,  1.9944e-01,\n",
            "            2.9363e-01,  6.3774e-01],\n",
            "          [ 2.0306e-01,  5.2354e-01,  1.4476e-01,  ...,  3.3407e-01,\n",
            "            3.4744e-01,  4.2310e-01],\n",
            "          [ 7.6222e-02,  2.3461e-01, -2.7653e-01,  ...,  1.7803e-01,\n",
            "            4.5952e-01,  3.3490e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0141e+00, -8.0138e-01, -5.6507e-01,  ..., -7.7024e-01,\n",
            "           -1.0719e+00, -8.7495e-01],\n",
            "          [-4.9290e-01, -8.5097e-01, -1.0618e+00,  ..., -6.6651e-01,\n",
            "           -7.5590e-01, -3.4735e-01],\n",
            "          [-6.5365e-01, -6.2131e-01, -9.2736e-01,  ..., -1.1643e+00,\n",
            "           -7.6357e-01, -1.1041e+00],\n",
            "          ...,\n",
            "          [-8.7569e-01, -7.4534e-01, -4.7489e-01,  ..., -2.9375e-01,\n",
            "           -5.2281e-01, -6.7064e-01],\n",
            "          [-8.5992e-01, -1.0042e+00, -7.4585e-01,  ..., -1.5730e-01,\n",
            "           -8.3174e-01, -1.6834e-01],\n",
            "          [-4.2135e-01, -8.2410e-01, -5.0390e-01,  ..., -5.3691e-01,\n",
            "           -3.1846e-01, -7.3660e-01]],\n",
            "\n",
            "         [[-8.8418e-02,  2.3933e-01, -1.6409e-01,  ..., -1.4724e-01,\n",
            "            2.7694e-01,  3.0763e-01],\n",
            "          [-2.8530e-02,  1.9888e-01, -4.5293e-01,  ...,  3.2243e-01,\n",
            "           -1.7208e-02, -3.1080e-02],\n",
            "          [-1.5077e-01,  3.2791e-01, -4.8500e-02,  ..., -4.1366e-02,\n",
            "           -2.9233e-01, -9.2004e-02],\n",
            "          ...,\n",
            "          [ 3.7636e-02,  1.7243e-01, -2.3756e-01,  ..., -4.3570e-01,\n",
            "            1.2051e-02,  2.3575e-03],\n",
            "          [-1.3316e-01,  8.9618e-02, -3.4055e-01,  ..., -1.5743e-01,\n",
            "           -1.2319e-01,  4.8710e-01],\n",
            "          [-4.3971e-01,  5.4073e-02,  4.4258e-03,  ...,  2.4557e-01,\n",
            "            5.1183e-01, -1.2303e-01]],\n",
            "\n",
            "         [[ 2.6390e-01,  3.1413e-01, -1.5092e-02,  ...,  2.7906e-01,\n",
            "           -5.0116e-01, -1.2620e-01],\n",
            "          [ 4.0564e-01, -1.2767e-01, -2.5412e-01,  ...,  2.0281e-01,\n",
            "           -9.0543e-04,  2.5917e-01],\n",
            "          [-1.5257e-03,  3.0746e-01,  6.3302e-02,  ..., -1.9736e-01,\n",
            "           -3.7713e-01, -2.9073e-02],\n",
            "          ...,\n",
            "          [-1.1174e-01, -7.9397e-02, -1.7469e-01,  ...,  2.3377e-01,\n",
            "           -1.1237e-01, -1.9321e-01],\n",
            "          [ 1.6083e-01,  1.7450e-01,  6.6016e-01,  ..., -1.3683e-01,\n",
            "            7.7815e-03, -3.6887e-01],\n",
            "          [ 2.4316e-01,  2.7723e-01,  3.2066e-01,  ...,  7.3609e-02,\n",
            "           -1.0496e-01, -4.1607e-01]]]], device='cuda:0')\n",
            "tensor([[-5.4962e-01, -5.5193e-01,  2.8013e-01,  1.6982e-01,  6.9291e-01,\n",
            "          1.5279e-01, -7.5247e-01,  2.6994e-02,  6.4402e-01,  4.8047e-01,\n",
            "         -6.6281e-01, -1.1992e-01,  5.2721e-01,  4.0236e-01, -5.9468e-01,\n",
            "          2.5393e-01, -3.0198e-02, -6.8007e-01, -5.3793e-01, -2.4707e-01,\n",
            "         -2.2217e-01,  3.0902e-01, -8.3646e-01, -5.3448e-02, -8.7915e-01,\n",
            "         -1.0564e+00,  5.2158e-01,  1.2330e+00, -1.3026e-02, -1.6406e-01,\n",
            "          4.3479e-01, -1.1010e-01, -5.0853e-01, -1.5571e-01,  1.4762e-02,\n",
            "          8.2884e-01, -8.6515e-01, -5.6658e-01, -4.5599e-01, -6.0118e-01,\n",
            "         -5.1347e-01, -1.4562e-01, -5.7359e-01,  4.0851e-01, -7.4983e-01,\n",
            "         -8.2420e-01, -1.8882e-01,  1.0082e-01,  2.9627e-01,  6.0168e-01,\n",
            "         -1.7690e+00, -5.8932e-02,  4.0646e-01, -4.6685e-01, -4.3787e-01,\n",
            "         -1.2310e+00, -4.6155e-01, -5.1276e-01, -5.4446e-01,  1.0017e+00,\n",
            "          1.1833e-01, -5.4578e-01, -2.9142e-01, -2.2983e-01,  5.5351e-01,\n",
            "         -1.9473e-01, -4.5915e-01,  6.4000e-01, -9.5803e-01, -5.4987e-01,\n",
            "          8.4730e-01,  4.1117e-01,  3.7816e-03,  5.8293e-01,  5.8124e-01,\n",
            "          6.1696e-01, -5.3067e-01,  1.0269e-01,  5.4611e-02, -2.2314e-01,\n",
            "          1.0593e+00,  7.0802e-01,  6.4875e-01,  8.9385e-01, -6.5580e-01,\n",
            "          4.0093e-01,  2.0084e-01,  4.7629e-01,  5.5011e-01, -6.0696e-01,\n",
            "          5.2165e-02, -2.4739e-01, -3.4956e-01,  4.0401e-01,  4.1505e-01,\n",
            "          9.4153e-01, -6.0765e-01, -9.0499e-01, -7.4841e-01, -3.7474e-01,\n",
            "          2.7992e-01,  7.5251e-01, -5.9603e-02, -4.9765e-01,  2.1763e-01,\n",
            "          6.2930e-01,  7.5298e-01,  1.7028e-01, -8.8865e-01, -1.4586e-02,\n",
            "         -3.0224e-01,  9.6176e-01, -1.3487e+00, -2.5697e-01, -9.1202e-01,\n",
            "          1.2172e+00, -6.3593e-01,  1.0030e-01,  4.6998e-01,  3.0300e-01,\n",
            "          2.0665e-01, -2.0955e-01,  7.1279e-01,  9.5963e-01,  5.7979e-01,\n",
            "         -4.8118e-02, -1.2601e-01,  3.3516e-01, -3.1165e-01,  1.4244e+00,\n",
            "          6.5381e-02, -8.6762e-01, -1.3283e-01,  8.9357e-02,  4.6951e-01,\n",
            "          1.8450e-02, -1.9665e-01,  1.3415e-01,  2.3167e-01, -6.5062e-01,\n",
            "         -1.4044e+00, -8.3175e-01,  4.4117e-01,  3.9557e-01,  2.7064e-01,\n",
            "         -8.0739e-01,  5.5391e-01,  5.9940e-01, -3.9012e-01,  4.9858e-01,\n",
            "         -4.9716e-01, -8.4724e-03,  8.7754e-01,  3.3987e-01, -3.6478e-01,\n",
            "         -1.0039e+00,  2.3072e-01, -5.3196e-01, -5.1160e-01,  1.1820e-01,\n",
            "          3.5642e-01, -3.2378e-01,  6.7412e-01,  3.4371e-01, -7.4196e-01,\n",
            "          3.8625e-01, -6.8070e-03, -8.0729e-01,  4.0781e-01, -1.9061e-01,\n",
            "         -1.7664e-01, -7.8225e-01,  8.0883e-02, -3.6043e-01, -2.0493e-01,\n",
            "         -2.8719e-01,  1.2544e+00,  5.4370e-02, -6.2252e-02, -3.5879e-01,\n",
            "          2.6506e-02,  5.9364e-01, -8.7821e-01,  1.3905e-03, -1.4538e-01,\n",
            "         -4.6043e-01, -1.1345e-03,  1.5005e-02,  5.0401e-01, -1.9835e-01,\n",
            "         -2.5196e-01,  7.9297e-01, -4.9986e-01, -5.2663e-01, -6.5633e-02,\n",
            "         -4.9074e-01, -4.1150e-01,  3.5722e-02, -7.4843e-01, -7.2831e-01,\n",
            "          2.1534e-01, -1.7199e-01,  4.5060e-01,  1.0816e+00,  3.9629e-01,\n",
            "         -1.2369e-01,  1.6208e-01, -1.5534e+00, -4.2257e-01, -4.6744e-01,\n",
            "          3.0271e-01, -3.7072e-01,  5.8658e-01, -1.7563e-01, -4.2696e-01,\n",
            "          2.4522e-01,  1.9360e-01,  6.8483e-02,  4.7968e-01,  6.1665e-01,\n",
            "         -5.3892e-01,  1.9631e-03, -7.4145e-02,  8.7445e-02, -6.3776e-01,\n",
            "          2.1355e-02,  2.4466e-01,  7.9202e-01, -3.0989e-01, -1.7692e-01,\n",
            "         -4.4915e-01, -9.0020e-01,  9.6623e-01, -1.4858e+00, -1.2980e+00,\n",
            "         -5.5173e-01, -1.8809e-01, -8.4010e-02,  1.3522e-01, -1.0121e+00,\n",
            "          1.2465e+00, -1.6356e+00,  4.6498e-01,  2.9470e-01, -7.0262e-01,\n",
            "         -5.9147e-01,  5.0881e-01,  4.1016e-01, -5.7297e-01, -9.6813e-02,\n",
            "         -5.1938e-01,  3.3078e-01,  8.5320e-01,  1.7480e-01, -4.1286e-01,\n",
            "         -4.7246e-01, -1.8358e-01, -4.4118e-01,  2.1343e-01,  3.9405e-01,\n",
            "         -6.1452e-01, -1.7360e-02,  3.6454e-01, -6.1167e-02, -5.6780e-01,\n",
            "          1.3117e-01,  6.5979e-02,  9.2731e-02, -7.8665e-01, -1.8863e-01,\n",
            "          3.1022e-01, -3.7801e-01,  1.5153e-01,  4.2506e-01, -7.1376e-01,\n",
            "          9.0497e-02,  2.3283e-01,  4.6412e-01,  4.8237e-01,  3.8738e-01,\n",
            "          3.1062e-01,  1.3218e-01,  7.0989e-01,  5.7724e-01, -2.0831e-01,\n",
            "         -7.8437e-02,  1.1923e-01, -1.6686e+00,  5.9759e-01,  3.6906e-01,\n",
            "         -4.3952e-01, -2.1326e-01,  1.0423e-01, -5.3788e-01, -4.8980e-01,\n",
            "         -2.8777e-01,  8.4774e-01, -8.6636e-02, -2.6569e-01, -1.2895e-01,\n",
            "         -3.2084e-01, -1.1665e-01,  6.4032e-02,  2.9034e-01,  3.4469e-01,\n",
            "         -1.9412e+00, -3.0548e-01, -6.9256e-01, -4.5323e-01, -3.5873e-01,\n",
            "          3.2049e-01, -1.5742e-01,  6.9847e-01,  3.2198e-02, -2.7840e-01,\n",
            "         -1.4568e-01, -3.2889e-02,  3.3613e-01,  9.5590e-01, -6.4586e-01,\n",
            "          9.6347e-01, -1.8114e-01, -4.5150e-01, -1.7916e-02,  1.8542e-01,\n",
            "          4.4110e-01,  3.6740e-01,  1.0238e+00,  8.0261e-01, -1.7082e-01,\n",
            "          1.0374e-01,  5.0597e-01, -1.0572e+00,  7.9595e-01,  4.0825e-01,\n",
            "         -7.3579e-01, -3.5692e-01, -2.5797e-02, -3.8645e-02, -2.8224e-01,\n",
            "         -7.5501e-01, -1.6711e-01,  9.8699e-01, -4.1174e-01, -3.8025e-01,\n",
            "          3.1320e-02, -9.2468e-01, -5.4439e-01, -1.9021e-01,  2.2376e-01,\n",
            "         -2.6916e-01,  7.7736e-01, -2.6643e-02, -3.7093e-01, -2.8511e-01,\n",
            "         -3.9931e-01,  2.9886e-01,  2.2874e-01, -1.7865e-01,  5.8202e-01,\n",
            "          1.6478e-01, -7.9199e-01, -1.7771e-01,  6.0859e-01, -6.2978e-02,\n",
            "         -1.9753e-01,  1.6051e-01,  4.2708e-01, -5.1749e-01,  4.9842e-01,\n",
            "         -5.9624e-02,  3.8626e-01,  8.1828e-02,  5.9975e-01,  6.2135e-01,\n",
            "          8.0934e-01, -2.5277e-01,  3.9595e-01, -3.2386e-01, -6.3822e-01,\n",
            "         -6.9754e-01, -6.6793e-01,  8.2127e-01, -2.4218e-01, -1.5975e-01,\n",
            "         -5.5377e-01,  2.2487e-01, -3.9460e-01,  1.3654e+00,  2.9971e-02,\n",
            "          2.9372e-01,  4.1783e-01, -1.4382e+00, -6.0254e-01,  3.9325e-01,\n",
            "         -1.1895e-02, -2.9326e-02,  2.5872e-02,  3.8930e-01,  8.0606e-01,\n",
            "         -1.0288e+00, -1.0357e+00,  9.9264e-01, -9.9554e-01, -1.1814e+00,\n",
            "          3.6901e-01, -8.6791e-01,  4.1055e-01, -6.3629e-01, -1.8691e-01,\n",
            "          1.9999e-01,  3.6285e-01,  1.4125e-01,  2.0992e-01, -2.1428e-01,\n",
            "         -3.3219e-01, -3.6291e-01, -1.6452e-01, -1.5699e+00, -6.6618e-01,\n",
            "          5.8427e-01, -9.9933e-01,  1.9138e-01, -6.0742e-01, -2.6875e-03,\n",
            "          2.8153e-01, -3.3879e-01,  3.4366e-01, -4.0192e-01, -7.6485e-01,\n",
            "         -1.0603e-02,  9.7386e-03,  1.6485e-01, -1.3334e+00,  2.7983e-01,\n",
            "         -4.1499e-01,  2.5053e-01,  4.9634e-01,  4.4734e-01, -2.3421e-01,\n",
            "          7.4188e-03, -9.2156e-01,  4.4451e-01, -3.1324e-01,  3.1029e-01,\n",
            "         -4.3469e-01,  3.4421e-01,  9.3127e-01,  2.1048e-01,  1.3110e+00,\n",
            "         -2.9227e-01, -2.9974e-01,  3.7686e-01, -5.3000e-01, -1.1746e+00,\n",
            "          9.4675e-03, -1.6191e-01, -3.8298e-02,  6.5413e-01,  2.9939e-01,\n",
            "          1.0819e+00,  2.3657e-01, -5.0991e-02,  2.9797e-01, -2.7965e-01,\n",
            "          4.4536e-01, -1.5040e-01, -5.0270e-01, -4.9514e-01,  4.4761e-01,\n",
            "         -4.9152e-01, -2.5810e-01, -1.1003e-01, -1.5394e-01,  2.4833e-01,\n",
            "         -8.6203e-01,  3.2608e-01, -1.2845e-01, -2.2027e-01, -5.3793e-01,\n",
            "          6.0579e-01, -1.0741e-01,  2.2735e-01, -2.7870e-01, -2.6746e-01,\n",
            "         -3.2191e-01, -9.2620e-01,  7.1158e-01, -4.2224e-02,  9.1560e-01,\n",
            "         -7.6579e-01,  7.0119e-01,  1.5803e-01,  1.6310e-01,  5.6986e-01,\n",
            "         -1.3747e-01, -1.2847e-01, -3.4685e-01,  2.6046e-01, -3.0617e-01,\n",
            "          1.3565e-01, -4.0921e-01,  6.7486e-02,  8.6719e-01, -3.0571e-01,\n",
            "         -4.3815e-01,  2.4429e-01, -1.5260e-01,  1.0227e+00,  3.7896e-01,\n",
            "         -5.1765e-01, -9.8349e-01, -5.8193e-01, -5.0674e-01,  6.1484e-01,\n",
            "         -7.8965e-01, -4.3015e-01,  9.7901e-01, -1.2874e+00, -3.1316e-01,\n",
            "          5.2143e-01, -5.1949e-01,  5.7406e-01,  9.6121e-01, -8.7238e-02,\n",
            "          7.2295e-01,  1.5909e+00, -7.5739e-01, -1.7546e-01, -5.4790e-02,\n",
            "          3.4107e-01,  5.0776e-01,  2.0534e-01,  1.0095e+00,  4.3276e-01,\n",
            "         -7.2810e-01, -4.7484e-01, -1.0669e-02,  7.5849e-01, -2.6041e-02,\n",
            "         -2.5754e-02, -2.4610e-01, -3.3986e-02,  4.3733e-01, -1.2802e-01,\n",
            "          1.9452e-01,  2.7079e-01,  3.2689e-01,  5.1745e-01,  1.1733e+00,\n",
            "          2.9265e-01, -1.2428e+00,  2.3313e-01, -1.0142e-01, -3.6046e-01,\n",
            "          5.7222e-01, -3.0785e-01, -9.0337e-01, -2.8777e-01, -1.3335e-01,\n",
            "         -4.5939e-01,  4.3952e-01,  1.0374e+00, -7.2144e-02, -4.1435e-01,\n",
            "         -5.9449e-01, -5.9189e-03, -4.0045e-01,  7.2115e-01, -3.9443e-01,\n",
            "         -1.0310e+00, -5.0481e-01, -5.1075e-01,  8.4641e-01,  2.9250e-01,\n",
            "         -6.0325e-01,  2.2116e-01, -3.8414e-01,  6.5986e-01, -3.3272e-01,\n",
            "          3.4607e-01, -9.5460e-01, -2.0460e+00, -2.2658e-01,  1.9805e-01,\n",
            "         -5.1421e-01,  1.5566e-01, -6.7439e-01,  1.2036e-01, -7.8253e-02,\n",
            "          7.5266e-01,  1.9796e-01,  2.0637e-01, -6.4212e-01, -7.9218e-02,\n",
            "         -3.4667e-01, -4.6718e-01,  2.9361e-01,  9.5205e-01, -4.2024e-01,\n",
            "         -1.0052e+00,  1.6609e+00, -7.1074e-01,  4.1323e-01, -1.1504e+00,\n",
            "         -5.7277e-01, -3.5166e-01,  4.5187e-01,  1.4090e+00,  3.7422e-01,\n",
            "          2.9074e-01, -7.1076e-01, -3.9320e-01,  1.1064e-02, -7.5413e-01,\n",
            "          1.0596e+00, -5.2435e-01, -2.6035e-01, -6.3147e-01, -5.4203e-03,\n",
            "          6.4464e-01,  8.3638e-01,  4.2354e-01,  5.6053e-01,  8.9888e-02,\n",
            "          7.6491e-02, -4.2967e-01, -1.0185e+00,  1.2959e-01, -1.1635e+00,\n",
            "         -1.7336e-01, -7.5569e-01, -5.3968e-01,  5.7662e-01,  4.2319e-01,\n",
            "          2.4991e-02,  6.5405e-01, -2.2392e-01, -7.1071e-01,  1.5205e-01,\n",
            "          8.7867e-02, -1.0196e-01,  8.5587e-02,  1.9096e-01,  1.3510e-01,\n",
            "         -1.8998e-02, -6.0687e-01,  8.8544e-01,  2.0140e-01,  4.7313e-02,\n",
            "         -1.4014e-01, -6.1652e-02,  1.5723e-01,  4.3238e-01,  5.8865e-01,\n",
            "          9.5941e-02,  3.7871e-01, -8.9741e-01, -1.7079e-01, -1.1395e+00,\n",
            "         -7.6532e-01, -1.3513e-01,  2.6060e-01, -8.4842e-01,  9.2026e-01,\n",
            "         -2.9804e-01, -7.0182e-01,  4.9482e-02,  1.7523e-01, -3.3106e-01,\n",
            "         -7.3380e-01, -9.5159e-01,  3.5316e-01,  1.3084e-01,  4.9358e-02,\n",
            "          9.0239e-01,  1.0616e+00,  4.7712e-01, -1.7039e-01,  8.0363e-01,\n",
            "          1.3350e+00, -5.2808e-01,  7.9816e-01,  7.7302e-01,  5.6925e-01,\n",
            "         -2.5020e-02,  3.0484e-01, -8.3055e-01, -1.2869e+00,  1.7147e-01,\n",
            "         -2.5827e-01, -8.3552e-01,  4.8684e-01, -2.1566e-01, -8.7000e-01,\n",
            "         -1.4260e-01, -6.4748e-02,  3.6766e-01,  7.1691e-01,  5.3261e-01,\n",
            "         -4.9025e-01, -3.9105e-01, -4.1321e-01, -6.5614e-01, -9.3897e-01,\n",
            "          5.9490e-01, -8.6142e-01, -2.5115e-01,  2.7266e-01,  1.0546e+00,\n",
            "          7.5232e-02,  8.0372e-03,  8.5611e-02, -7.4152e-01, -6.7980e-01,\n",
            "         -2.3338e-01, -6.8399e-01, -9.8663e-01, -6.4424e-01, -7.1191e-02,\n",
            "         -2.2412e-01,  7.6861e-01,  2.9235e-01,  5.8393e-01,  5.4136e-03,\n",
            "         -1.7521e-01, -5.5715e-01,  8.1475e-02,  4.4301e-01, -8.5876e-01,\n",
            "         -1.3925e+00, -3.8691e-01, -1.1930e+00, -9.6472e-01, -2.0994e-02,\n",
            "         -3.3528e-01, -1.7879e-01, -1.0527e+00,  1.8071e-01,  8.5555e-02,\n",
            "          9.5380e-01, -6.8138e-01,  5.8793e-01, -1.8202e-01,  1.1289e+00,\n",
            "         -4.4201e-01, -7.7032e-02,  3.6230e-01,  6.2461e-01,  2.8959e-01,\n",
            "         -7.2345e-01,  5.9050e-01, -1.8742e-01, -1.2203e-01, -3.3835e-01,\n",
            "          3.9266e-01,  9.4233e-01, -1.2082e+00,  1.2472e+00,  1.0935e+00,\n",
            "          2.5035e-02, -1.1890e-01, -3.5310e-01,  3.9918e-03,  8.4366e-01,\n",
            "         -1.4083e-01, -4.9808e-01,  6.6102e-01, -7.8011e-02,  5.4283e-01,\n",
            "         -1.0142e-01,  7.1124e-01,  2.8790e-01,  1.1527e+00,  1.1640e+00,\n",
            "          2.3778e-01, -5.7812e-01,  6.0542e-02, -9.2203e-01, -2.3118e-01,\n",
            "         -6.5938e-01,  3.0745e-01,  3.2286e-01, -1.3934e-02, -2.5885e-02,\n",
            "          1.0107e-01, -6.3524e-01, -3.0030e-03, -7.8988e-02, -8.5377e-01,\n",
            "         -4.5373e-01,  7.5880e-01, -4.6053e-01, -7.3416e-01,  3.3557e-01,\n",
            "         -2.2900e-01,  2.4049e-01,  1.3473e+00, -2.6761e-01, -2.0064e-01,\n",
            "         -4.3683e-02,  9.7437e-02,  3.6053e-01,  7.8252e-01, -4.6657e-01,\n",
            "         -1.3904e-01, -9.4162e-01, -1.2111e-01,  8.7122e-02, -6.8920e-02,\n",
            "         -7.1529e-01, -7.7195e-01, -3.8584e-01, -5.6179e-02,  1.0793e+00,\n",
            "          2.3339e-01,  4.6982e-01,  6.4360e-01,  2.9936e-01, -1.5058e-01,\n",
            "          6.8519e-01,  2.6185e-01, -1.7119e-01, -4.1338e-01, -1.4934e-02,\n",
            "         -8.8904e-02,  3.6414e-01,  1.4270e-01, -4.0509e-01,  3.5312e-01,\n",
            "         -2.7258e-01,  4.6480e-01, -1.4031e-01, -2.8820e-02, -2.9380e-01,\n",
            "         -7.0469e-01, -4.6582e-01, -2.3581e-01,  1.3683e-01, -1.6526e-01,\n",
            "          4.2147e-01, -5.1591e-01, -1.7260e-01,  1.0579e+00, -2.9860e-01,\n",
            "          5.0400e-01,  5.2065e-01,  4.7605e-01,  5.2924e-01,  4.9166e-01,\n",
            "          4.9366e-01,  1.4417e-01,  3.1509e-01,  7.1148e-01, -1.6663e-01,\n",
            "          3.0739e-01,  2.0276e-01, -1.7265e-01,  1.0253e-01,  3.4084e-01,\n",
            "          1.9302e-01, -2.4760e-01,  6.6651e-02,  3.2256e-01,  4.8387e-01,\n",
            "          4.1119e-01,  4.3027e-01,  6.5213e-02,  1.4106e-01,  4.8885e-02,\n",
            "         -7.5537e-01,  3.0120e-01,  4.9554e-01,  6.2765e-01, -2.9642e-01,\n",
            "         -7.0143e-02,  5.9630e-01,  1.0150e+00,  3.9325e-01, -1.1301e-04,\n",
            "         -4.1764e-01, -3.2269e-01,  7.9294e-01,  1.0435e-01, -4.8310e-01,\n",
            "          1.5351e+00, -3.2486e-01,  8.7767e-02, -5.5833e-01, -2.8853e-01,\n",
            "          1.1941e+00,  3.3392e-01,  9.2431e-02,  7.9935e-01,  1.4687e+00,\n",
            "         -6.7320e-01,  5.4176e-01,  1.0703e-02,  1.8377e-01,  3.1496e-01,\n",
            "          1.8832e-01,  3.0874e-01, -2.3937e-01, -7.4789e-01,  2.7336e-01,\n",
            "          3.2054e-02,  4.5466e-01,  1.2142e+00, -5.0593e-01,  3.8888e-01,\n",
            "          5.3419e-01, -1.0707e+00, -1.6626e-02, -4.9046e-01,  6.9544e-01,\n",
            "         -1.0937e+00,  2.9458e-01,  5.4903e-01,  1.7326e-01,  1.7152e-01,\n",
            "          4.2952e-01,  8.1343e-01, -5.7852e-02,  7.5984e-01,  6.2221e-01,\n",
            "         -3.4753e-01,  2.3793e-01, -4.8129e-01, -3.1717e-01, -5.9866e-01,\n",
            "          6.8229e-01, -2.1481e-01,  4.2949e-01,  5.5666e-01,  5.8670e-01,\n",
            "          6.7701e-02,  7.2035e-02, -3.4751e-01,  9.9127e-02,  7.2532e-01,\n",
            "         -2.5522e-01, -1.0924e-01, -1.9446e-01,  1.8531e-01, -3.6715e-01,\n",
            "          1.1051e+00, -8.0144e-01, -7.0540e-01,  2.6223e-01,  7.7635e-01,\n",
            "          5.1711e-01, -5.0316e-01,  2.2852e-01,  1.9371e-01,  1.4326e-01,\n",
            "         -8.0854e-01, -1.3427e-01,  5.7493e-01,  2.8268e-02,  1.0364e+00,\n",
            "         -4.8564e-01, -7.0990e-02,  1.0404e-01,  5.9222e-01, -5.9453e-01,\n",
            "         -3.7472e-01,  2.5917e-01,  5.9217e-01,  1.0091e+00,  7.6524e-02,\n",
            "         -5.2481e-02, -4.2519e-01, -7.8821e-01,  1.1754e+00, -3.7070e-01,\n",
            "         -8.7198e-02,  1.3046e+00, -4.1774e-01,  5.6767e-01, -9.8869e-02,\n",
            "         -6.7834e-01, -6.5656e-01,  1.1094e-01,  5.9278e-01, -3.0989e-01,\n",
            "          8.8791e-01,  2.7371e-01,  5.0210e-01, -4.2799e-01, -4.3118e-01,\n",
            "         -2.3217e-01,  4.0725e-01, -1.2866e+00, -1.5934e+00, -1.2505e+00,\n",
            "         -4.1189e-02, -1.9193e-01, -7.1424e-01,  7.2985e-01, -4.1529e-01]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "x = torch.randn(1, 3, 224, 224).to(device)  # Example input tensor\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(x)\n",
        "    \n",
        "print(output)  # Should print the output of the Mask R-CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Classification : ImageNet1K\n",
        " \n",
        "Train Like CSwin\n",
        "\n",
        "- 300 epochs\n",
        "- 224 x 224\n",
        "- AdamW with Weight decay of 0.05\n",
        "- batch size 1024\n",
        "- Lr 0.001\n",
        "- Cosine Lr scheduler 20 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageNet\n",
        "from torchvision.datasets.folder import default_loader\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "import torch.amp\n",
        "import timm\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 300\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 1024\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.05\n",
        "WARMUP_EPOCHS = 20\n",
        "NUM_WORKERS = 8  # Number of workers for DataLoader\n",
        "NUM_CLASSES = 1000  # Number of classes in ImageNet1K\n",
        "DATA_DIR = '/imagenet'  # Update with your ImageNet dataset path\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# --- Augmentation-specific hyperparameters ---\n",
        "RAND_AUGMENT_NUM_OPS = 2\n",
        "RAND_AUGMENT_MAGNITUDE = 9\n",
        "MIXUP_ALPHA = 0.8\n",
        "CUTMIX_ALPHA = 1.0\n",
        "RANDOM_ERASING_PROB = 0.25\n",
        "\n",
        "model = HyneterForFPN(hidden_dim=96, Conv_layers=(2, 2, 2, 2), TB_layers=(2, 2, 2, 2), heads=(3, 6, 12, 24), num_classes=NUM_CLASSES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "criterion  = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=WARMUP_EPOCHS)\n",
        "scaler = torch.GradScaler()  # For mixed precision training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data Transformations with timm's built-in options ---\n",
        "train_transform = timm.data.create_transform(\n",
        "    input_size=IMAGE_SIZE,\n",
        "    is_training=True,\n",
        "    color_jitter=0.4,\n",
        "    auto_augment=f'rand-n{RAND_AUGMENT_NUM_OPS}-m{RAND_AUGMENT_MAGNITUDE}',\n",
        "    interpolation='bicubic',\n",
        "    mean=(0.485, 0.456, 0.406),\n",
        "    std=(0.229, 0.224, 0.225),\n",
        "    re_prob=RANDOM_ERASING_PROB,\n",
        "    re_mode='pixel',\n",
        "    re_count=1,\n",
        ")\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = ImageNet(root=DATA_DIR, split='train', transform=train_transform)\n",
        "val_dataset = ImageNet(root=DATA_DIR, split='val', transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from timm.data import Mixup\n",
        "\n",
        "mixup_fn = None\n",
        "if MIXUP_ALPHA > 0 or CUTMIX_ALPHA > 0:\n",
        "    mixup_fn = Mixup(\n",
        "        mixup_alpha=MIXUP_ALPHA,\n",
        "        cutmix_alpha=CUTMIX_ALPHA,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        prob=1.0,\n",
        "        switch_prob=0.5,\n",
        "        mode='batch',\n",
        "        label_smoothing=0.1,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR = './checkpoints' # Directory to save checkpoints\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True) # Create the directory if it doesn't exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} started at: {time.ctime(epoch_start_time)}\")\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        if mixup_fn is not None:\n",
        "            inputs, labels = mixup_fn(inputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
        "                  f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_dataset) # Approximate for mixed samples\n",
        "    print(f\"Epoch {epoch+1} Train Loss: {epoch_train_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_correct_predictions = 0\n",
        "    val_total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total_samples += labels.size(0)\n",
        "            val_correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = val_running_loss / len(val_dataset)\n",
        "    val_accuracy = val_correct_predictions / val_total_samples\n",
        "    print(f\"Epoch {epoch+1} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Record epoch end time and calculate duration\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "    print(f\"Epoch {epoch+1} ended at: {time.ctime(epoch_end_time)}\")\n",
        "    print(f\"Epoch {epoch+1} duration: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'epoch_{epoch+1:03d}.pth')\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(), # Save scheduler state too\n",
        "        'best_accuracy': best_accuracy, # Or current accuracy if you want to track more\n",
        "        'val_accuracy': val_accuracy,\n",
        "        'val_loss': val_loss,\n",
        "        'train_loss': epoch_train_loss,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Save the best model\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), f'custom_imagenet_best_model.pth')\n",
        "        print(f\"Saved best model with accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nTraining finished :D (I wish)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNDbBQJm_ld4"
      },
      "source": [
        "# Trainning\n",
        "\n",
        "Optimizer : AdamW (lr = 0.00001, Weight decay = 0.05)\n",
        "with MMdetection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV3HXVeTkw_O"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as dset\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = get_hyneter_mask_rcnn_model(hyneter_base_fpn())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = hyneter_base()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = get_hyneter_mask_rcnn_model(hyneter_base_fpn())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = hyneter_base_fpn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "x = torch.randn(1, 3, 224, 224).to(device)  # Example input tensor\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(x)\n",
        "    \n",
        "print(output)  # Should print the output of the Mask R-CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = dset.CocoDetection(root='data/train2017', annFile='data/annotations/instances_train2017.json', transform=transform)\n",
        "val_dataset  = dset.CocoDetection(root='data/val2017', annFile='data/annotations/instances_val2017.json', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        )\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = 91  # 80 COCO classes + 1 background\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001, weight_decay=0.05)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = hyneter_base()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hyneter",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
